{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-01T17:46:09.538441Z",
     "start_time": "2025-02-01T17:46:05.850882Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def generate_basic_item_hour_log(\n",
    "    data_path, \n",
    "    output_path,\n",
    "    datasets=(\"train\", \"dev\", \"test\")\n",
    "):\n",
    "    \"\"\"\n",
    "    Step A1:\n",
    "      Generates a basic ItemHourLog.csv from MIND data files (train/dev/test).\n",
    "      Each row => (item_id, hour_offset, exposure, dataset).\n",
    "      We'll skip 'clicked' in the hour log. We'll simply count how many rows => exposure.\n",
    "      If you do have a 'clicked' column, you can incorporate it.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    global_earliest_time = None\n",
    "    all_logs = []\n",
    "\n",
    "    # 1) find earliest timestamp across all splits\n",
    "    for ds in datasets:\n",
    "        csv_file = os.path.join(data_path, f\"{ds}.csv\")\n",
    "        if not os.path.exists(csv_file):\n",
    "            print(f\"[WARN] {csv_file} not found. Skipping {ds}.\")\n",
    "            continue\n",
    "        df = pd.read_csv(csv_file, sep=\"\\t\")\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"], unit=\"s\", errors=\"coerce\")\n",
    "        tmin = df[\"time\"].min()\n",
    "        if global_earliest_time is None or tmin < global_earliest_time:\n",
    "            global_earliest_time = tmin\n",
    "\n",
    "    if global_earliest_time is None:\n",
    "        print(\"[ERROR] No valid data found. Aborting.\")\n",
    "        return\n",
    "\n",
    "    # 2) For each dataset, group by (item, hour_offset)\n",
    "    for ds in datasets:\n",
    "        csv_file = os.path.join(data_path, f\"{ds}.csv\")\n",
    "        if not os.path.exists(csv_file):\n",
    "            continue\n",
    "\n",
    "        print(f\"[INFO] Processing {ds} ...\")\n",
    "        df = pd.read_csv(csv_file, sep=\"\\t\")\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"], unit=\"s\", errors=\"coerce\")\n",
    "        df[\"timelevel\"] = df[\"time\"].dt.floor(\"h\")\n",
    "\n",
    "        # hour_offset from earliest\n",
    "        df[\"hour_offset\"] = (\n",
    "            (df[\"timelevel\"] - global_earliest_time).dt.total_seconds() // 3600\n",
    "        ).astype(int)\n",
    "\n",
    "        # Count how many rows => \"exposure\"\n",
    "        # if you want to incorporate 'clicked', do it separately\n",
    "        item_hour_df = (\n",
    "            df.groupby([\"item_id\", \"hour_offset\"], as_index=False)\n",
    "            .agg(exposure=(\"user_id\", \"count\"))\n",
    "        )\n",
    "        item_hour_df[\"dataset\"] = ds\n",
    "        all_logs.append(item_hour_df)\n",
    "\n",
    "    if not all_logs:\n",
    "        print(\"[WARN] No logs found.\")\n",
    "        return\n",
    "\n",
    "    final = pd.concat(all_logs, ignore_index=True)\n",
    "    final.sort_values([\"item_id\", \"hour_offset\"], inplace=True)\n",
    "\n",
    "    outfile = os.path.join(output_path, \"ItemHourLog_basic.csv\")\n",
    "    final.to_csv(outfile, index=False)\n",
    "    print(f\"[INFO] Wrote basic item-hour logs to {outfile}.\")\n",
    "\n",
    "def compute_vitality_for_item_hour_log(\n",
    "    input_csv,\n",
    "    output_csv,\n",
    "    beta_E=0.5,\n",
    "    beta_NE=0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Step A2:\n",
    "      Reads the basic item-hour log from input_csv,\n",
    "      for each hour_offset, we rank items by a user feedback metric (exposure or CTR).\n",
    "      Then compute vitality = rank_percentile - beta_E if exposure>0, else -beta_NE.\n",
    "      We'll store the hour-by-hour 'vitality' in a new column, plus a cumulative vitality.\n",
    "      The paper references thresholding on the cumulative sum for \"death,\" but we'll \n",
    "      store it here so the next script can handle it.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "    # We'll assume 'exposure' is the measure of user feedback. \n",
    "    # If you have actual CTR or click_count, you'd rank by that.\n",
    "\n",
    "    # rank items *within each hour_offset* by exposure\n",
    "    # then compute percentile rank. For each hour_offset => sort items by exposure\n",
    "    # We can do a groupby on hour_offset\n",
    "    all_rows = []\n",
    "    for hour_off, group in df.groupby(\"hour_offset\", sort=True):\n",
    "        # sort by exposure descending\n",
    "        sorted_g = group.sort_values(\"exposure\", ascending=False)\n",
    "        n = len(sorted_g)\n",
    "        # rank percentile => rank(i) / n\n",
    "        # let's do zero-based rank\n",
    "        sorted_g[\"rank\"] = np.arange(n)\n",
    "        # rank_percentile = 1 - (rank / (n-1)) if we want top exposure => percentile near 1\n",
    "        # or we can do rank / (n-1)\n",
    "        # We'll define: rank_percentile = 1 - (rank / (n-1)) so that item with highest exposure => percentile=1\n",
    "        if n>1:\n",
    "            sorted_g[\"rank_percentile\"] = 1.0 - (sorted_g[\"rank\"]/(n-1))\n",
    "        else:\n",
    "            sorted_g[\"rank_percentile\"] = 1.0\n",
    "\n",
    "        all_rows.append(sorted_g)\n",
    "\n",
    "    newdf = pd.concat(all_rows, ignore_index=True)\n",
    "    # define vitality\n",
    "    def compute_vitality(row):\n",
    "        if row[\"exposure\"]>0:\n",
    "            return row[\"rank_percentile\"] - beta_E\n",
    "        else:\n",
    "            return -beta_NE\n",
    "\n",
    "    newdf[\"vitality\"] = newdf.apply(compute_vitality, axis=1)\n",
    "    newdf.sort_values([\"item_id\",\"hour_offset\"], inplace=True)\n",
    "\n",
    "    newdf.to_csv(output_csv, index=False)\n",
    "    print(f\"[INFO] Wrote vitality-based itemHourLog to {output_csv}.\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    DATA_PATH = \"\"\n",
    "    OUTPUT_PATH = \"./output\"\n",
    "\n",
    "    # Step A1\n",
    "    generate_basic_item_hour_log(DATA_PATH, OUTPUT_PATH, datasets=[\"train\",\"val\",\"test\"])\n",
    "    # Step A2\n",
    "    compute_vitality_for_item_hour_log(\n",
    "        input_csv=os.path.join(OUTPUT_PATH, \"ItemHourLog_basic.csv\"),\n",
    "        output_csv=os.path.join(OUTPUT_PATH, \"ItemHourLog.csv\"),\n",
    "        beta_E=0.5,\n",
    "        beta_NE=0.5\n",
    "    )\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] val.csv not found. Skipping val.\n",
      "[INFO] Processing train ...\n",
      "[INFO] Processing test ...\n",
      "[INFO] Wrote basic item-hour logs to ./output/ItemHourLog_basic.csv.\n",
      "[INFO] Wrote vitality-based itemHourLog to ./output/ItemHourLog.csv.\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
