{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-01T22:03:06.921996Z",
     "start_time": "2025-02-01T21:57:15.607904Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import log2\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "#######################################################\n",
    "# 1) Data loading with negative samples\n",
    "#######################################################\n",
    "\n",
    "def load_mind_data_with_neg(train_csv, val_csv, test_csv):\n",
    "    \"\"\"\n",
    "    We assume train.csv might have columns: user_id, item_id, time, neg_items, etc.\n",
    "    We will parse them:\n",
    "      - For each row => (user, item, label=1)\n",
    "      - For each item in 'neg_items' => (user, neg_item, label=0)\n",
    "    Return dataframes for train, val, test with [user_id, item_id, time, label].\n",
    "    \"\"\"\n",
    "    def parse_dataset(filename):\n",
    "        data_pos = []\n",
    "        data_neg = []\n",
    "        if not os.path.exists(filename):\n",
    "            return pd.DataFrame(columns=[\"user_id\",\"item_id\",\"time\",\"label\"])\n",
    "        df = pd.read_csv(filename, sep=\"\\t\")\n",
    "        # If there's no 'neg_items' col => no negative sampling\n",
    "        if \"neg_items\" not in df.columns:\n",
    "            # fallback => treat all as label=1?\n",
    "            df[\"label\"] = 1\n",
    "            return df[[\"user_id\",\"item_id\",\"time\",\"label\"]]\n",
    "        # parse neg_items\n",
    "        for row in df.itertuples(index=False):\n",
    "            user = getattr(row,\"user_id\")\n",
    "            item = getattr(row,\"item_id\")\n",
    "            tval = getattr(row,\"time\")\n",
    "            # label=1\n",
    "            data_pos.append((user,item,tval,1))\n",
    "            # read neg_items => string of format \"[7856, 8058, ...]\"\n",
    "            s = getattr(row,\"neg_items\")\n",
    "            # parse them\n",
    "            s = s.strip()\n",
    "            s = s.lstrip(\"[\").rstrip(\"]\")\n",
    "            if len(s)>0:\n",
    "                parts = s.split(\",\")\n",
    "                for neg_str in parts:\n",
    "                    neg_str=neg_str.strip()\n",
    "                    if neg_str:\n",
    "                        neg_id = int(neg_str)\n",
    "                        data_neg.append((user,neg_id,tval,0))\n",
    "        df_pos = pd.DataFrame(data_pos, columns=[\"user_id\",\"item_id\",\"time\",\"label\"])\n",
    "        df_neg = pd.DataFrame(data_neg, columns=[\"user_id\",\"item_id\",\"time\",\"label\"])\n",
    "        finaldf = pd.concat([df_pos, df_neg], ignore_index=True)\n",
    "        return finaldf\n",
    "\n",
    "    train_df = parse_dataset(train_csv)\n",
    "    val_df   = parse_dataset(val_csv)\n",
    "    test_df  = parse_dataset(test_csv)\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "class MindInteractionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Basic PyTorch dataset with pointwise (user, item, label).\n",
    "    \"\"\"\n",
    "    def __init__(self, df, user2idx, item2idx):\n",
    "        self.users = df[\"user_id\"].map(user2idx).values\n",
    "        self.items = df[\"item_id\"].map(item2idx).values\n",
    "        self.labels= df[\"label\"].values.astype(float)\n",
    "        self.times = df[\"time\"].values.astype(int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.users[idx],\n",
    "            self.items[idx],\n",
    "            self.labels[idx],\n",
    "            self.times[idx]\n",
    "        )\n",
    "\n",
    "#######################################################\n",
    "# 2) NeuMF model\n",
    "#######################################################\n",
    "\n",
    "class NeuMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_dim=8, mlp_hidden=16):\n",
    "        super().__init__()\n",
    "        self.user_emb_gmf = nn.Embedding(num_users, emb_dim)\n",
    "        self.item_emb_gmf = nn.Embedding(num_items, emb_dim)\n",
    "\n",
    "        self.user_emb_mlp = nn.Embedding(num_users, emb_dim)\n",
    "        self.item_emb_mlp = nn.Embedding(num_items, emb_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2*emb_dim, mlp_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden, mlp_hidden//2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.final = nn.Linear(emb_dim + mlp_hidden//2, 1)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.user_emb_gmf.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb_gmf.weight)\n",
    "        nn.init.xavier_uniform_(self.user_emb_mlp.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb_mlp.weight)\n",
    "\n",
    "    def forward(self, user_idx, item_idx):\n",
    "        u_gmf = self.user_emb_gmf(user_idx) \n",
    "        i_gmf = self.item_emb_gmf(item_idx)\n",
    "        gmf_out= u_gmf*i_gmf\n",
    "\n",
    "        u_mlp = self.user_emb_mlp(user_idx)\n",
    "        i_mlp = self.item_emb_mlp(item_idx)\n",
    "        mlp_in= torch.cat([u_mlp, i_mlp], dim=1)\n",
    "        mlp_out= self.mlp(mlp_in)\n",
    "\n",
    "        concat = torch.cat([gmf_out, mlp_out], dim=1)\n",
    "        logit  = self.final(concat)\n",
    "        return logit.view(-1)\n",
    "\n",
    "#######################################################\n",
    "# 3) Training loop\n",
    "#######################################################\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    for batch in loader:\n",
    "        users, items, labels, _times = batch\n",
    "        users = users.to(device)\n",
    "        items = items.to(device)\n",
    "        labels= labels.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(users, items)\n",
    "        loss = loss_fn(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+= loss.item()*len(labels)\n",
    "    return total_loss/len(loader.dataset)\n",
    "\n",
    "def eval_one_epoch(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss=0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            users, items, labels, _times = batch\n",
    "            users= users.to(device)\n",
    "            items= items.to(device)\n",
    "            labels= labels.float().to(device)\n",
    "            preds= model(users, items)\n",
    "            loss= loss_fn(preds, labels)\n",
    "            total_loss+= loss.item()*len(labels)\n",
    "    return total_loss/len(loader.dataset)\n",
    "\n",
    "#######################################################\n",
    "# 4) Combine with GRV, Evaluate Coverage/HR@10\n",
    "#######################################################\n",
    "\n",
    "def load_cox_data_and_survival(cox_data_csv, cox_survival_csv, itemHourLog_csv):\n",
    "    cox_df = pd.read_csv(cox_data_csv)\n",
    "    if \"T_i0\" in cox_df.columns:\n",
    "        t0_map = dict(zip(cox_df[\"item_id\"], cox_df[\"T_i0\"]))\n",
    "    else:\n",
    "        hour_df= pd.read_csv(itemHourLog_csv)\n",
    "        tmp= hour_df.groupby(\"item_id\")[\"hour_offset\"].min().reset_index()\n",
    "        t0_map= dict(zip(tmp[\"item_id\"], tmp[\"hour_offset\"]))\n",
    "\n",
    "    surv_df= pd.read_csv(cox_survival_csv)\n",
    "    grv_cols= [c for c in surv_df.columns if c.startswith(\"GRV_t\")]\n",
    "    def parse_off(col):\n",
    "        return int(col.split(\"t\")[-1])\n",
    "\n",
    "    item_grv={}\n",
    "    for row in surv_df.itertuples(index=False):\n",
    "        it= getattr(row,\"item_id\")\n",
    "        d= {}\n",
    "        for c in grv_cols:\n",
    "            val= getattr(row,c)\n",
    "            off= parse_off(c)\n",
    "            d[off]= val\n",
    "        item_grv[it]= d\n",
    "\n",
    "    cox_map={}\n",
    "    for it_id in item_grv:\n",
    "        T0= t0_map[it_id] if it_id in t0_map else 0\n",
    "        cox_map[it_id]={\n",
    "            \"T_i0\": T0,\n",
    "            \"grv_map\": item_grv[it_id]\n",
    "        }\n",
    "    return cox_map\n",
    "\n",
    "def get_grv(cox_map, item_id, current_hour, default_val=0.0):\n",
    "    if item_id not in cox_map:\n",
    "        return default_val\n",
    "    T0= cox_map[item_id][\"T_i0\"]\n",
    "    offset= int(current_hour - T0)\n",
    "    if offset<=0:\n",
    "        return 0.0\n",
    "    grv_map= cox_map[item_id][\"grv_map\"]\n",
    "    offsets= sorted(grv_map.keys())\n",
    "    if offset< offsets[0]:\n",
    "        offset= offsets[0]\n",
    "    if offset> offsets[-1]:\n",
    "        offset= offsets[-1]\n",
    "    return grv_map.get(offset, default_val)\n",
    "\n",
    "def evaluate_ranking(\n",
    "    model, df_test, user2idx, item2idx, cox_map,\n",
    "    gamma=0.0, K=10, device=torch.device(\"cpu\"), new_threshold=100\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate ranking performance using HR@K, NDCG@K, Coverage@K, and New Item Coverage@K.\n",
    "    \"\"\"\n",
    "    df_test = df_test.copy()\n",
    "    df_test[\"time_hr\"] = (df_test[\"time\"] // 3600).astype(int)\n",
    "\n",
    "    grouped = df_test.groupby(\"user_id\")\n",
    "    coverage_items = set()\n",
    "    new_item_hits = 0  \n",
    "\n",
    "    hits_at_k = 0\n",
    "    ndcg_at_k = 0\n",
    "    total_positives = 0\n",
    "\n",
    "    all_users = list(grouped.groups.keys())\n",
    "    all_item_ids = list(item2idx.keys())\n",
    "\n",
    "    rng = np.random.default_rng(0)\n",
    "\n",
    "    for user_id in tqdm(all_users, desc=\"EvaluateRanking\"):\n",
    "        g = grouped.get_group(user_id)\n",
    "        t_hr = g[\"time_hr\"].min()  # Earliest request\n",
    "\n",
    "        # Build a candidate set: positive items + 50 random items\n",
    "        pos_items = g[g[\"label\"] == 1][\"item_id\"].unique()\n",
    "        candidate_items = np.unique(np.concatenate([pos_items, rng.choice(all_item_ids, size=50, replace=False)]))\n",
    "\n",
    "        # Convert candidate items to indices, filtering valid ones\n",
    "        valid_candidates = [(it, item2idx[it]) for it in candidate_items if it in item2idx]\n",
    "        if not valid_candidates:\n",
    "            continue  \n",
    "\n",
    "        item_indices = [idx for _, idx in valid_candidates]\n",
    "        item_ids = [it for it, _ in valid_candidates]\n",
    "\n",
    "        # Compute model scores\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            user_tensor = torch.tensor([user2idx[user_id]] * len(item_indices), dtype=torch.long, device=device)\n",
    "            item_tensor = torch.tensor(item_indices, dtype=torch.long, device=device)\n",
    "            preds = model(user_tensor, item_tensor)\n",
    "        base_scores = preds.cpu().numpy().flatten()\n",
    "\n",
    "        # Compute final scores using GRV\n",
    "        final_scores = []\n",
    "        for i, (it, base_score) in enumerate(zip(item_ids, base_scores)):\n",
    "            grv_val = get_grv(cox_map, it, t_hr)\n",
    "            final_score = (1 - gamma) * base_score + gamma * grv_val\n",
    "            final_scores.append(final_score)\n",
    "\n",
    "        # Select top-K items\n",
    "        top_indices = np.argsort(-np.array(final_scores))[:K]\n",
    "        top_items = [item_ids[i] for i in top_indices]\n",
    "\n",
    "        # **Coverage Calculation**\n",
    "        coverage_items.update(top_items)\n",
    "\n",
    "        # **New Item Coverage Calculation (Fixed)**\n",
    "        new_items = set(it for it in top_items if it in cox_map and cox_map[it][\"T_i0\"] >= new_threshold)\n",
    "        if new_items:\n",
    "            new_item_hits += 1  # Count users who received at least one new item\n",
    "\n",
    "        # **HR & NDCG Calculation**\n",
    "        total_positives += len(pos_items)\n",
    "        hits = sum(1 for pos_it in pos_items if pos_it in top_items)\n",
    "        hits_at_k += hits\n",
    "\n",
    "        # Compute NDCG\n",
    "        dcg = sum(1.0 / log2(np.where(np.array(top_items) == pos_it)[0][0] + 2) for pos_it in pos_items if pos_it in top_items)\n",
    "        idcg = sum(1.0 / log2(i + 2) for i in range(len(pos_items))) if len(pos_items) > 0 else 0\n",
    "        ndcg_at_k += dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "    # **Normalize Metrics**\n",
    "    hr = hits_at_k / total_positives if total_positives > 0 else 0\n",
    "    ndcg = ndcg_at_k / len(all_users) if len(all_users) > 0 else 0\n",
    "    coverage = len(coverage_items) / len(item2idx)  # ✅ Normalize by total items\n",
    "    new_item_coverage = new_item_hits / len(all_users)  # ✅ Normalize by total users\n",
    "\n",
    "    return hr, ndcg, coverage, new_item_coverage\n",
    "\n",
    "#######################################################\n",
    "# 5) Main experiment\n",
    "#######################################################\n",
    "def main_experiment(\n",
    "    train_csv=\"train.csv\",\n",
    "    val_csv=\"val.csv\",\n",
    "    test_csv=\"test.csv\",\n",
    "    cox_data_csv=\"./cox_output/cox_data.csv\",\n",
    "    cox_survival_csv=\"./cox_output/cox_survival.csv\",\n",
    "    itemHourLog_csv=\"./output/ItemHourLog.csv\",\n",
    "    gamma=0.0,\n",
    "    epochs=3,\n",
    "    batch_size=256,\n",
    "    emb_dim=8,\n",
    "    mlp_hidden=16,\n",
    "    use_cuda=True\n",
    "):\n",
    "\n",
    "    device= torch.device(\"cuda:0\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] device={device}\")\n",
    "\n",
    "    # 1) Load data with negative sampling\n",
    "    train_df, val_df, test_df= load_mind_data_with_neg(train_csv, val_csv, test_csv)\n",
    "    print(f\"[INFO] train={len(train_df)}, val={len(val_df)}, test={len(test_df)}\")\n",
    "\n",
    "    # Build global user/item index\n",
    "    all_users= pd.concat([train_df[\"user_id\"], val_df[\"user_id\"], test_df[\"user_id\"]]).unique()\n",
    "    all_items= pd.concat([train_df[\"item_id\"], val_df[\"item_id\"], test_df[\"item_id\"]]).unique()\n",
    "    user2idx= {u:i for i,u in enumerate(all_users)}\n",
    "    item2idx= {i:u for u,i in enumerate(all_items)}\n",
    "\n",
    "    # 2) Build PyTorch datasets\n",
    "    train_ds= MindInteractionDataset(train_df, user2idx, item2idx)\n",
    "    val_ds= MindInteractionDataset(val_df, user2idx, item2idx)\n",
    "    test_ds= MindInteractionDataset(test_df, user2idx, item2idx)\n",
    "\n",
    "    train_loader= DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader  = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # 3) Build NeuMF\n",
    "    model= NeuMF(len(user2idx), len(item2idx), emb_dim, mlp_hidden).to(device)\n",
    "    optimizer= optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn= nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # 4) Train\n",
    "    for ep in range(epochs):\n",
    "        tr_loss= train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "        vl_loss= eval_one_epoch(model, val_loader,   loss_fn, device)\n",
    "        print(f\"Epoch {ep}: train_loss={tr_loss:.4f}, val_loss={vl_loss:.4f}\")\n",
    "\n",
    "    # 5) Load cox data => cox_map\n",
    "    cox_map= load_cox_data_and_survival(\n",
    "        cox_data_csv, cox_survival_csv, itemHourLog_csv\n",
    "    )\n",
    "\n",
    "    # 6) Evaluate with GRV => final\n",
    "    hr, ndcg, cov, newcov= evaluate_ranking(\n",
    "        model, test_df, user2idx, item2idx, cox_map,\n",
    "        gamma=gamma,\n",
    "        K=10,\n",
    "        device=device,\n",
    "        new_threshold=100\n",
    "    )\n",
    "    print(f\"[RESULT] HR@10={hr:.4f}, NDCG@10={ndcg:.4f}, coverage@10={cov}, new_item_coverage@10={newcov}\")\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main_experiment(\n",
    "        train_csv=\"train.csv\",\n",
    "        val_csv=\"dev.csv\",\n",
    "        test_csv=\"test.csv\",\n",
    "        cox_data_csv=\"./cox_output/cox_data.csv\",\n",
    "        cox_survival_csv=\"./cox_output/cox_survival.csv\",\n",
    "        itemHourLog_csv=\"./output/ItemHourLog.csv\",\n",
    "        gamma=0,\n",
    "        epochs=3,\n",
    "        batch_size=256,\n",
    "        emb_dim=8,\n",
    "        mlp_hidden=16,\n",
    "        use_cuda=True\n",
    "    )\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] device=cpu\n",
      "[INFO] train=2480957, val=17976500, test=10479700\n",
      "Epoch 0: train_loss=0.0073, val_loss=14.2270\n",
      "Epoch 1: train_loss=0.0000, val_loss=17.8839\n",
      "Epoch 2: train_loss=0.0000, val_loss=19.2211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EvaluateRanking: 100%|██████████| 57900/57900 [00:53<00:00, 1076.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] HR@10=0.2057, NDCG@10=0.1162, coverage@10=0.38042463835744283, new_item_coverage@10=0.9444559585492228\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3e48028c3bb1717d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
