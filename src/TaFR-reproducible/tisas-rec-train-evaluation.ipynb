{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-01T20:06:37.079776Z",
     "start_time": "2025-02-01T19:51:37.637661Z"
    }
   },
   "source": [
    "from src.experiment import load_mind_data_with_neg\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "#######################################################\n",
    "# 1) Load GRV Data (Item Popularity Over Time)\n",
    "#######################################################\n",
    "\n",
    "def load_cox_data_and_survival(cox_data_csv, cox_survival_csv, itemHourLog_csv):\n",
    "    cox_df = pd.read_csv(cox_data_csv)\n",
    "    if \"T_i0\" in cox_df.columns:\n",
    "        t0_map = dict(zip(cox_df[\"item_id\"], cox_df[\"T_i0\"]))\n",
    "    else:\n",
    "        hour_df = pd.read_csv(itemHourLog_csv)\n",
    "        tmp = hour_df.groupby(\"item_id\")[\"hour_offset\"].min().reset_index()\n",
    "        t0_map = dict(zip(tmp[\"item_id\"], tmp[\"hour_offset\"]))\n",
    "\n",
    "    surv_df = pd.read_csv(cox_survival_csv)\n",
    "    grv_cols = [c for c in surv_df.columns if c.startswith(\"GRV_t\")]\n",
    "    \n",
    "    def parse_off(col):\n",
    "        return int(col.split(\"t\")[-1])\n",
    "\n",
    "    item_grv = {}\n",
    "    for row in surv_df.itertuples(index=False):\n",
    "        it = getattr(row, \"item_id\")\n",
    "        d = {parse_off(c): getattr(row, c) for c in grv_cols}\n",
    "        item_grv[it] = d\n",
    "\n",
    "    cox_map = {it_id: {\"T_i0\": t0_map.get(it_id, 0), \"grv_map\": item_grv[it_id]} for it_id in item_grv}\n",
    "    return cox_map\n",
    "\n",
    "def get_grv(cox_map, item_id, current_hour, default_val=0.0):\n",
    "    if item_id not in cox_map:\n",
    "        return default_val\n",
    "    T0 = cox_map[item_id][\"T_i0\"]\n",
    "    offset = int(current_hour - T0)\n",
    "    if offset <= 0:\n",
    "        return 0.0\n",
    "    grv_map = cox_map[item_id][\"grv_map\"]\n",
    "    offsets = sorted(grv_map.keys())\n",
    "    offset = min(max(offset, offsets[0]), offsets[-1])\n",
    "    return grv_map.get(offset, default_val)\n",
    "\n",
    "#######################################################\n",
    "# 2) TiSASRec Model (Time-Aware Self-Attention)\n",
    "#######################################################\n",
    "\n",
    "class TiSASRec(nn.Module):\n",
    "    def __init__(self, num_items, emb_dim=16, num_heads=2, num_layers=2, max_seq_len=50, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len  # Ensure max length is set\n",
    "        self.item_emb = nn.Embedding(num_items, emb_dim)\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, emb_dim)\n",
    "        self.time_emb = nn.Linear(1, emb_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=num_heads, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(emb_dim, num_items)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def forward(self, item_seq, time_seq):\n",
    "        batch_size, seq_len = item_seq.shape\n",
    "        if seq_len > self.max_seq_len:\n",
    "            item_seq = item_seq[:, -self.max_seq_len:]  # Truncate to max_seq_len\n",
    "            time_seq = time_seq[:, -self.max_seq_len:]  # Truncate to match\n",
    "\n",
    "        item_embeddings = self.item_emb(item_seq)\n",
    "        pos_indices = torch.arange(item_seq.shape[1], device=item_seq.device).unsqueeze(0).expand_as(item_seq)\n",
    "        pos_embeddings = self.pos_emb(pos_indices)\n",
    "        time_embeddings = self.time_emb(time_seq.unsqueeze(-1))\n",
    "\n",
    "        seq_embeddings = item_embeddings + pos_embeddings + time_embeddings\n",
    "        seq_embeddings = self.encoder(seq_embeddings)\n",
    "        last_output = seq_embeddings[:, -1, :]\n",
    "        return self.fc(self.dropout(last_output))\n",
    "\n",
    "class TimeAwareSessionDataset(Dataset):\n",
    "    def __init__(self, df, item2idx, session_length=5):\n",
    "        self.sessions = []\n",
    "        self.time_diffs = []\n",
    "        grouped = df.groupby(\"user_id\")\n",
    "        for _, group in grouped:\n",
    "            group = group.sort_values(\"time\")\n",
    "            items = group[\"item_id\"].values\n",
    "            times = group[\"time\"].values\n",
    "            if len(items) < session_length:\n",
    "                continue\n",
    "            for i in range(len(items) - session_length + 1):\n",
    "                session_seq = items[i: i + session_length]\n",
    "                time_seq = times[i: i + session_length] - times[i]\n",
    "                indices = [item2idx[it] for it in session_seq if it in item2idx]\n",
    "                if len(indices) == session_length:\n",
    "                    self.sessions.append(indices)\n",
    "                    self.time_diffs.append(time_seq)\n",
    "        self.sessions = np.array(self.sessions)\n",
    "        self.time_diffs = np.array(self.time_diffs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sessions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        session = self.sessions[idx]\n",
    "        time_diff = self.time_diffs[idx]\n",
    "        return torch.tensor(session[:-1], dtype=torch.long), torch.tensor(time_diff[:-1], dtype=torch.float), torch.tensor(session[-1], dtype=torch.long)\n",
    "\n",
    "#######################################################\n",
    "# 3) Training TiSASRec\n",
    "#######################################################\n",
    "\n",
    "def train_one_epoch_tisasrec(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for item_seq, time_seq, target in loader:\n",
    "        item_seq, time_seq, target = item_seq.to(device), time_seq.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(item_seq, time_seq)\n",
    "        loss = loss_fn(logits, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * item_seq.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "#######################################################\n",
    "# 4) GRV-Based Ranking for TiSASRec\n",
    "#######################################################\n",
    "\n",
    "def evaluate_tisasrec_ranking(model, test_df, item2idx, cox_map, gamma=0.3, K=10, device=\"cpu\"):\n",
    "    test_df = test_df.copy()\n",
    "    test_df[\"time_hr\"] = (test_df[\"time\"] // 3600).astype(int)\n",
    "    grouped = test_df.groupby(\"user_id\")\n",
    "    coverage_items, newcov = set(), 0  # `newcov` starts as a float\n",
    "    hits_at_k, ndcg_at_k, total_positives = 0, 0, 0\n",
    "    all_users = list(grouped.groups.keys())\n",
    "    all_item_ids = test_df[\"item_id\"].unique()\n",
    "    rng = np.random.default_rng(0)\n",
    "\n",
    "    for user_id in tqdm(all_users, desc=\"Evaluating TiSASRec with GRV\"):\n",
    "        g = grouped.get_group(user_id)\n",
    "        t_hr = g[\"time_hr\"].min()\n",
    "        pos_items = g[g[\"label\"] == 1][\"item_id\"].unique()\n",
    "        candidate_items = np.unique(np.concatenate([pos_items, rng.choice(all_item_ids, size=50, replace=False)]))\n",
    "\n",
    "        valid_items = [item2idx[it] for it in candidate_items if it in item2idx]\n",
    "        if not valid_items:\n",
    "            continue  \n",
    "        if len(valid_items) > model.max_seq_len:\n",
    "            valid_items = valid_items[-model.max_seq_len:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = torch.tensor(valid_items, dtype=torch.long, device=device).unsqueeze(0)\n",
    "            time_inputs = torch.zeros_like(inputs, dtype=torch.float)\n",
    "            logits = model(inputs, time_inputs)\n",
    "            base_scores = logits.cpu().numpy().flatten()\n",
    "\n",
    "        final_scores = [(1 - gamma) * base_score + gamma * get_grv(cox_map, it, t_hr) for it, base_score in zip(candidate_items, base_scores)]\n",
    "        top_items = candidate_items[np.argsort(-np.array(final_scores))[:K]]\n",
    "\n",
    "        # ✅ Normalize Coverage\n",
    "        coverage_items.update(top_items)\n",
    "\n",
    "        # ✅ Normalize New Item Coverage\n",
    "        new_item_count = sum(1 for it in top_items if it in cox_map and cox_map[it][\"T_i0\"] >= 100)\n",
    "        newcov += new_item_count / K  # Proportion of new items recommended\n",
    "\n",
    "        total_positives += len(pos_items)\n",
    "        hits = sum(1 for pos_it in pos_items if pos_it in top_items)\n",
    "        hits_at_k += hits\n",
    "        ndcg_at_k += sum(1.0 / np.log2(np.where(top_items == pos_it)[0][0] + 2) for pos_it in pos_items if pos_it in top_items)\n",
    "\n",
    "    hr = hits_at_k / total_positives if total_positives > 0 else 0\n",
    "    ndcg = ndcg_at_k / len(all_users) if len(all_users) > 0 else 0\n",
    "    newcov /= len(all_users)  # Normalize new coverage by total users\n",
    "    coverage = len(coverage_items) / len(item2idx)  # Normalize coverage by total items\n",
    "\n",
    "    return hr, ndcg, coverage, newcov  # ✅ Coverage is now a proportion\n",
    "\n",
    "#######################################################\n",
    "# 5) Main Experiment for TiSASRec with GRV\n",
    "#######################################################\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load data using the correct function\n",
    "    train_df, val_df, test_df = load_mind_data_with_neg(\"train.csv\", \"dev.csv\", \"test.csv\")\n",
    "\n",
    "    # Debug: Print column names for verification\n",
    "    print(f\"[DEBUG] Columns in train.csv: {train_df.columns}\")\n",
    "\n",
    "    # Ensure 'item_id' column exists\n",
    "    if 'item_id' not in train_df.columns:\n",
    "        raise KeyError(\"Column 'item_id' is missing from train.csv. Check the column names.\")\n",
    "\n",
    "    # Build item2idx mapping\n",
    "    all_items = pd.concat([train_df[\"item_id\"], val_df[\"item_id\"], test_df[\"item_id\"]]).unique()\n",
    "    item2idx = {i: idx for idx, i in enumerate(all_items)}\n",
    "\n",
    "    # Load GRV data\n",
    "    cox_map = load_cox_data_and_survival(\n",
    "        \"cox_output/cox_data.csv\", \n",
    "        \"cox_output/cox_survival.csv\", \n",
    "        \"output/ItemHourLog.csv\"\n",
    "    )\n",
    "\n",
    "    # Build training dataset and loader\n",
    "    train_ds = TimeAwareSessionDataset(train_df, item2idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "\n",
    "    # Define model\n",
    "    model = TiSASRec(len(item2idx)).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train the model\n",
    "    for ep in range(3):\n",
    "        train_one_epoch_tisasrec(model, train_loader, optimizer, loss_fn, device)\n",
    "\n",
    "    # **Evaluate with GRV-based ranking**\n",
    "    hr, ndcg, cov, newcov = evaluate_tisasrec_ranking(\n",
    "        model, test_df, item2idx, cox_map, gamma=0.1, K=10, device=device\n",
    "    )\n",
    "\n",
    "    print(f\"[RESULT] HR@10={hr:.4f}, NDCG@10={ndcg:.4f}, coverage@10={cov}, new_item_coverage@10={newcov}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Columns in train.csv: Index(['user_id', 'item_id', 'time', 'label'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating TiSASRec with GRV:   0%|          | 0/57900 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>=' not supported between instances of 'dict' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 227\u001B[0m\n\u001B[1;32m    224\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[RESULT] HR@10=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhr\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, NDCG@10=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mndcg\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, coverage@10=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcov\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, new_item_coverage@10=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnewcov\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 227\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[10], line 220\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m    217\u001B[0m     train_one_epoch_tisasrec(model, train_loader, optimizer, loss_fn, device)\n\u001B[1;32m    219\u001B[0m \u001B[38;5;66;03m# **Evaluate with GRV-based ranking**\u001B[39;00m\n\u001B[0;32m--> 220\u001B[0m hr, ndcg, cov, newcov \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_tisasrec_ranking\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    221\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mitem2idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcox_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgamma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mK\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\n\u001B[1;32m    222\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    224\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[RESULT] HR@10=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhr\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, NDCG@10=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mndcg\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, coverage@10=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcov\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, new_item_coverage@10=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnewcov\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[10], line 168\u001B[0m, in \u001B[0;36mevaluate_tisasrec_ranking\u001B[0;34m(model, test_df, item2idx, cox_map, gamma, K, device)\u001B[0m\n\u001B[1;32m    165\u001B[0m top_items \u001B[38;5;241m=\u001B[39m candidate_items[np\u001B[38;5;241m.\u001B[39margsort(\u001B[38;5;241m-\u001B[39mnp\u001B[38;5;241m.\u001B[39marray(final_scores))[:K]]\n\u001B[1;32m    167\u001B[0m coverage_items\u001B[38;5;241m.\u001B[39mupdate(top_items)\n\u001B[0;32m--> 168\u001B[0m \u001B[43mcoverage_new\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mit\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mit\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtop_items\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mit\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcox_map\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mand\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcox_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43mit\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m>\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    169\u001B[0m total_positives \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(pos_items)\n\u001B[1;32m    170\u001B[0m hits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m pos_it \u001B[38;5;129;01min\u001B[39;00m pos_items \u001B[38;5;28;01mif\u001B[39;00m pos_it \u001B[38;5;129;01min\u001B[39;00m top_items)\n",
      "Cell \u001B[0;32mIn[10], line 168\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    165\u001B[0m top_items \u001B[38;5;241m=\u001B[39m candidate_items[np\u001B[38;5;241m.\u001B[39margsort(\u001B[38;5;241m-\u001B[39mnp\u001B[38;5;241m.\u001B[39marray(final_scores))[:K]]\n\u001B[1;32m    167\u001B[0m coverage_items\u001B[38;5;241m.\u001B[39mupdate(top_items)\n\u001B[0;32m--> 168\u001B[0m coverage_new\u001B[38;5;241m.\u001B[39mupdate(it \u001B[38;5;28;01mfor\u001B[39;00m it \u001B[38;5;129;01min\u001B[39;00m top_items \u001B[38;5;28;01mif\u001B[39;00m it \u001B[38;5;129;01min\u001B[39;00m cox_map \u001B[38;5;129;01mand\u001B[39;00m \u001B[43mcox_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43mit\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m>\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m100\u001B[39;49m)\n\u001B[1;32m    169\u001B[0m total_positives \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(pos_items)\n\u001B[1;32m    170\u001B[0m hits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m pos_it \u001B[38;5;129;01min\u001B[39;00m pos_items \u001B[38;5;28;01mif\u001B[39;00m pos_it \u001B[38;5;129;01min\u001B[39;00m top_items)\n",
      "\u001B[0;31mTypeError\u001B[0m: '>=' not supported between instances of 'dict' and 'int'"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ebf274f595768b63"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
