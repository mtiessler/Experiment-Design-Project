{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Experiment Reproduction Pipeline",
   "id": "5d4966640c37cf67"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Requirements install",
   "id": "b51574e31c16782"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "!pip install requirements.txt"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1) Item Hour Log generation from ReChorus TOPK generated MIND dataset",
   "id": "1862fb2b8f3e674f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!python 1_item_hour_log_from_ReChorus.py",
   "id": "9663c9d13250308e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2) GRV generation from COX model",
   "id": "157acba30abce593"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T08:39:37.937406Z",
     "start_time": "2025-02-02T08:39:35.159751Z"
    }
   },
   "cell_type": "code",
   "source": "!python 2_COX_GRV.py",
   "id": "2965d9515ef79d8e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Max\\Experiment-Design-Project\\src\\TaFR-reproducible\\2_COX_GRV.py\", line 149, in <module>\n",
      "    train_and_generate_grv_with_vitality(\n",
      "  File \"C:\\Users\\Max\\Experiment-Design-Project\\src\\TaFR-reproducible\\2_COX_GRV.py\", line 27, in train_and_generate_grv_with_vitality\n",
      "    df = pd.read_csv(item_hour_log_csv)\n",
      "  File \"C:\\Users\\Max\\Experiment-Design-Project\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1026, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"C:\\Users\\Max\\Experiment-Design-Project\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 620, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"C:\\Users\\Max\\Experiment-Design-Project\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1620, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "  File \"C:\\Users\\Max\\Experiment-Design-Project\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1880, in _make_engine\n",
      "    self.handles = get_handle(\n",
      "  File \"C:\\Users\\Max\\Experiment-Design-Project\\venv\\lib\\site-packages\\pandas\\io\\common.py\", line 873, in get_handle\n",
      "    handle = open(\n",
      "FileNotFoundError: [Errno 2] No such file or directory: './output/ItemHourLog.csv'\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3) Backbone Models applying GAMMA = 0 and GAMMA = 0.3, 0.1",
   "id": "36df66435f201268"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T08:53:59.989659Z",
     "start_time": "2025-02-02T08:53:56.664272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from math import log2"
   ],
   "id": "d87d32d39511577f",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptim\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01moptim\u001B[39;00m\n",
      "File \u001B[1;32m~\\Experiment-Design-Project\\venv\\lib\\site-packages\\torch\\__init__.py:262\u001B[0m\n\u001B[0;32m    258\u001B[0m                     \u001B[38;5;28;01mraise\u001B[39;00m err\n\u001B[0;32m    260\u001B[0m         kernel32\u001B[38;5;241m.\u001B[39mSetErrorMode(prev_error_mode)\n\u001B[1;32m--> 262\u001B[0m     \u001B[43m_load_dll_libraries\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    263\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m _load_dll_libraries\n\u001B[0;32m    266\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_preload_cuda_deps\u001B[39m(lib_folder: \u001B[38;5;28mstr\u001B[39m, lib_name: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Experiment-Design-Project\\venv\\lib\\site-packages\\torch\\__init__.py:238\u001B[0m, in \u001B[0;36m_load_dll_libraries\u001B[1;34m()\u001B[0m\n\u001B[0;32m    236\u001B[0m is_loaded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    237\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m with_load_library_flags:\n\u001B[1;32m--> 238\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mkernel32\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLoadLibraryExW\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdll\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0x00001100\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    239\u001B[0m     last_error \u001B[38;5;241m=\u001B[39m ctypes\u001B[38;5;241m.\u001B[39mget_last_error()\n\u001B[0;32m    240\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m res \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m last_error \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m126\u001B[39m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.1) NeuMF",
   "id": "1fd4a351f469f976"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#######################################################\n",
    "# 1) Data loading with negative samples\n",
    "#######################################################\n",
    "\n",
    "def load_mind_data_with_neg(train_csv, val_csv, test_csv):\n",
    "    \"\"\"\n",
    "    We assume train.csv might have columns: user_id, item_id, time, neg_items, etc.\n",
    "    We will parse them:\n",
    "      - For each row => (user, item, label=1)\n",
    "      - For each item in 'neg_items' => (user, neg_item, label=0)\n",
    "    Return dataframes for train, val, test with [user_id, item_id, time, label].\n",
    "    \"\"\"\n",
    "    def parse_dataset(filename):\n",
    "        data_pos = []\n",
    "        data_neg = []\n",
    "        if not os.path.exists(filename):\n",
    "            return pd.DataFrame(columns=[\"user_id\",\"item_id\",\"time\",\"label\"])\n",
    "        df = pd.read_csv(filename, sep=\"\\t\")\n",
    "        # If there's no 'neg_items' col => no negative sampling\n",
    "        if \"neg_items\" not in df.columns:\n",
    "            # fallback => treat all as label=1?\n",
    "            df[\"label\"] = 1\n",
    "            return df[[\"user_id\",\"item_id\",\"time\",\"label\"]]\n",
    "        # parse neg_items\n",
    "        for row in df.itertuples(index=False):\n",
    "            user = getattr(row,\"user_id\")\n",
    "            item = getattr(row,\"item_id\")\n",
    "            tval = getattr(row,\"time\")\n",
    "            # label=1\n",
    "            data_pos.append((user,item,tval,1))\n",
    "            # read neg_items => string of format \"[7856, 8058, ...]\"\n",
    "            s = getattr(row,\"neg_items\")\n",
    "            # parse them\n",
    "            s = s.strip()\n",
    "            s = s.lstrip(\"[\").rstrip(\"]\")\n",
    "            if len(s)>0:\n",
    "                parts = s.split(\",\")\n",
    "                for neg_str in parts:\n",
    "                    neg_str=neg_str.strip()\n",
    "                    if neg_str:\n",
    "                        neg_id = int(neg_str)\n",
    "                        data_neg.append((user,neg_id,tval,0))\n",
    "        df_pos = pd.DataFrame(data_pos, columns=[\"user_id\",\"item_id\",\"time\",\"label\"])\n",
    "        df_neg = pd.DataFrame(data_neg, columns=[\"user_id\",\"item_id\",\"time\",\"label\"])\n",
    "        finaldf = pd.concat([df_pos, df_neg], ignore_index=True)\n",
    "        return finaldf\n",
    "\n",
    "    train_df = parse_dataset(train_csv)\n",
    "    val_df   = parse_dataset(val_csv)\n",
    "    test_df  = parse_dataset(test_csv)\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "class MindInteractionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Basic PyTorch dataset with pointwise (user, item, label).\n",
    "    \"\"\"\n",
    "    def __init__(self, df, user2idx, item2idx):\n",
    "        self.users = df[\"user_id\"].map(user2idx).values\n",
    "        self.items = df[\"item_id\"].map(item2idx).values\n",
    "        self.labels= df[\"label\"].values.astype(float)\n",
    "        self.times = df[\"time\"].values.astype(int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.users[idx],\n",
    "            self.items[idx],\n",
    "            self.labels[idx],\n",
    "            self.times[idx]\n",
    "        )\n",
    "\n",
    "#######################################################\n",
    "# 2) NeuMF model\n",
    "#######################################################\n",
    "\n",
    "class NeuMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_dim=8, mlp_hidden=16):\n",
    "        super().__init__()\n",
    "        self.user_emb_gmf = nn.Embedding(num_users, emb_dim)\n",
    "        self.item_emb_gmf = nn.Embedding(num_items, emb_dim)\n",
    "\n",
    "        self.user_emb_mlp = nn.Embedding(num_users, emb_dim)\n",
    "        self.item_emb_mlp = nn.Embedding(num_items, emb_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2*emb_dim, mlp_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden, mlp_hidden//2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.final = nn.Linear(emb_dim + mlp_hidden//2, 1)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.user_emb_gmf.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb_gmf.weight)\n",
    "        nn.init.xavier_uniform_(self.user_emb_mlp.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb_mlp.weight)\n",
    "\n",
    "    def forward(self, user_idx, item_idx):\n",
    "        u_gmf = self.user_emb_gmf(user_idx)\n",
    "        i_gmf = self.item_emb_gmf(item_idx)\n",
    "        gmf_out= u_gmf*i_gmf\n",
    "\n",
    "        u_mlp = self.user_emb_mlp(user_idx)\n",
    "        i_mlp = self.item_emb_mlp(item_idx)\n",
    "        mlp_in= torch.cat([u_mlp, i_mlp], dim=1)\n",
    "        mlp_out= self.mlp(mlp_in)\n",
    "\n",
    "        concat = torch.cat([gmf_out, mlp_out], dim=1)\n",
    "        logit  = self.final(concat)\n",
    "        return logit.view(-1)\n",
    "\n",
    "#######################################################\n",
    "# 3) Training loop\n",
    "#######################################################\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    for batch in loader:\n",
    "        users, items, labels, _times = batch\n",
    "        users = users.to(device)\n",
    "        items = items.to(device)\n",
    "        labels= labels.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(users, items)\n",
    "        loss = loss_fn(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+= loss.item()*len(labels)\n",
    "    return total_loss/len(loader.dataset)\n",
    "\n",
    "def eval_one_epoch(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss=0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            users, items, labels, _times = batch\n",
    "            users= users.to(device)\n",
    "            items= items.to(device)\n",
    "            labels= labels.float().to(device)\n",
    "            preds= model(users, items)\n",
    "            loss= loss_fn(preds, labels)\n",
    "            total_loss+= loss.item()*len(labels)\n",
    "    return total_loss/len(loader.dataset)\n",
    "\n",
    "#######################################################\n",
    "# 4) Combine with GRV, Evaluate Coverage/HR@10\n",
    "#######################################################\n",
    "\n",
    "def load_cox_data_and_survival(cox_data_csv, cox_survival_csv, itemHourLog_csv):\n",
    "    cox_df = pd.read_csv(cox_data_csv)\n",
    "    if \"T_i0\" in cox_df.columns:\n",
    "        t0_map = dict(zip(cox_df[\"item_id\"], cox_df[\"T_i0\"]))\n",
    "    else:\n",
    "        hour_df= pd.read_csv(itemHourLog_csv)\n",
    "        tmp= hour_df.groupby(\"item_id\")[\"hour_offset\"].min().reset_index()\n",
    "        t0_map= dict(zip(tmp[\"item_id\"], tmp[\"hour_offset\"]))\n",
    "\n",
    "    surv_df= pd.read_csv(cox_survival_csv)\n",
    "    grv_cols= [c for c in surv_df.columns if c.startswith(\"GRV_t\")]\n",
    "    def parse_off(col):\n",
    "        return int(col.split(\"t\")[-1])\n",
    "\n",
    "    item_grv={}\n",
    "    for row in surv_df.itertuples(index=False):\n",
    "        it= getattr(row,\"item_id\")\n",
    "        d= {}\n",
    "        for c in grv_cols:\n",
    "            val= getattr(row,c)\n",
    "            off= parse_off(c)\n",
    "            d[off]= val\n",
    "        item_grv[it]= d\n",
    "\n",
    "    cox_map={}\n",
    "    for it_id in item_grv:\n",
    "        T0= t0_map[it_id] if it_id in t0_map else 0\n",
    "        cox_map[it_id]={\n",
    "            \"T_i0\": T0,\n",
    "            \"grv_map\": item_grv[it_id]\n",
    "        }\n",
    "    return cox_map\n",
    "\n",
    "def get_grv(cox_map, item_id, current_hour, default_val=0.0):\n",
    "    if item_id not in cox_map:\n",
    "        return default_val\n",
    "    T0= cox_map[item_id][\"T_i0\"]\n",
    "    offset= int(current_hour - T0)\n",
    "    if offset<=0:\n",
    "        return 0.0\n",
    "    grv_map= cox_map[item_id][\"grv_map\"]\n",
    "    offsets= sorted(grv_map.keys())\n",
    "    if offset< offsets[0]:\n",
    "        offset= offsets[0]\n",
    "    if offset> offsets[-1]:\n",
    "        offset= offsets[-1]\n",
    "    return grv_map.get(offset, default_val)\n",
    "\n",
    "def evaluate_ranking(\n",
    "    model, df_test, user2idx, item2idx, cox_map,\n",
    "    gamma=0.0, K=10, device=torch.device(\"cpu\"), new_threshold=100\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate ranking performance using HR@K, NDCG@K, Coverage@K, and New Item Coverage@K.\n",
    "    \"\"\"\n",
    "    df_test = df_test.copy()\n",
    "    df_test[\"time_hr\"] = (df_test[\"time\"] // 3600).astype(int)\n",
    "\n",
    "    grouped = df_test.groupby(\"user_id\")\n",
    "    coverage_items = set()\n",
    "    new_item_hits = 0\n",
    "\n",
    "    hits_at_k = 0\n",
    "    ndcg_at_k = 0\n",
    "    total_positives = 0\n",
    "\n",
    "    all_users = list(grouped.groups.keys())\n",
    "    all_item_ids = list(item2idx.keys())\n",
    "\n",
    "    rng = np.random.default_rng(0)\n",
    "\n",
    "    for user_id in tqdm(all_users, desc=\"EvaluateRanking\"):\n",
    "        g = grouped.get_group(user_id)\n",
    "        t_hr = g[\"time_hr\"].min()  # Earliest request\n",
    "\n",
    "        # Build a candidate set: positive items + 50 random items\n",
    "        pos_items = g[g[\"label\"] == 1][\"item_id\"].unique()\n",
    "        candidate_items = np.unique(np.concatenate([pos_items, rng.choice(all_item_ids, size=50, replace=False)]))\n",
    "\n",
    "        # Convert candidate items to indices, filtering valid ones\n",
    "        valid_candidates = [(it, item2idx[it]) for it in candidate_items if it in item2idx]\n",
    "        if not valid_candidates:\n",
    "            continue\n",
    "\n",
    "        item_indices = [idx for _, idx in valid_candidates]\n",
    "        item_ids = [it for it, _ in valid_candidates]\n",
    "\n",
    "        # Compute model scores\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            user_tensor = torch.tensor([user2idx[user_id]] * len(item_indices), dtype=torch.long, device=device)\n",
    "            item_tensor = torch.tensor(item_indices, dtype=torch.long, device=device)\n",
    "            preds = model(user_tensor, item_tensor)\n",
    "        base_scores = preds.cpu().numpy().flatten()\n",
    "\n",
    "        # Compute final scores using GRV\n",
    "        final_scores = []\n",
    "        for i, (it, base_score) in enumerate(zip(item_ids, base_scores)):\n",
    "            grv_val = get_grv(cox_map, it, t_hr)\n",
    "            final_score = (1 - gamma) * base_score + gamma * grv_val\n",
    "            final_scores.append(final_score)\n",
    "\n",
    "        # Select top-K items\n",
    "        top_indices = np.argsort(-np.array(final_scores))[:K]\n",
    "        top_items = [item_ids[i] for i in top_indices]\n",
    "\n",
    "        # **Coverage Calculation**\n",
    "        coverage_items.update(top_items)\n",
    "\n",
    "        # **New Item Coverage Calculation (Fixed)**\n",
    "        new_items = set(it for it in top_items if it in cox_map and cox_map[it][\"T_i0\"] >= new_threshold)\n",
    "        if new_items:\n",
    "            new_item_hits += 1  # Count users who received at least one new item\n",
    "\n",
    "        # **HR & NDCG Calculation**\n",
    "        total_positives += len(pos_items)\n",
    "        hits = sum(1 for pos_it in pos_items if pos_it in top_items)\n",
    "        hits_at_k += hits\n",
    "\n",
    "        # Compute NDCG\n",
    "        dcg = sum(1.0 / log2(np.where(np.array(top_items) == pos_it)[0][0] + 2) for pos_it in pos_items if pos_it in top_items)\n",
    "        idcg = sum(1.0 / log2(i + 2) for i in range(len(pos_items))) if len(pos_items) > 0 else 0\n",
    "        ndcg_at_k += dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "    # **Normalize Metrics**\n",
    "    hr = hits_at_k / total_positives if total_positives > 0 else 0\n",
    "    ndcg = ndcg_at_k / len(all_users) if len(all_users) > 0 else 0\n",
    "    coverage = len(coverage_items) / len(item2idx)  # ✅ Normalize by total items\n",
    "    new_item_coverage = new_item_hits / len(all_users)  # ✅ Normalize by total users\n",
    "\n",
    "    return hr, ndcg, coverage, new_item_coverage\n",
    "\n",
    "#######################################################\n",
    "# 5) Main experiment\n",
    "#######################################################\n",
    "def neumf_experiment(\n",
    "    train_csv=\"train.csv\",\n",
    "    val_csv=\"val.csv\",\n",
    "    test_csv=\"test.csv\",\n",
    "    cox_data_csv=\"./cox_output/cox_data.csv\",\n",
    "    cox_survival_csv=\"./cox_output/cox_survival.csv\",\n",
    "    itemHourLog_csv=\"./output/ItemHourLog.csv\",\n",
    "    gamma=0.0,\n",
    "    epochs=3,\n",
    "    batch_size=256,\n",
    "    emb_dim=8,\n",
    "    mlp_hidden=16,\n",
    "    use_cuda=True\n",
    "):\n",
    "\n",
    "    device= torch.device(\"cuda:0\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] device={device}\")\n",
    "\n",
    "    # 1) Load data with negative sampling\n",
    "    train_df, val_df, test_df= load_mind_data_with_neg(train_csv, val_csv, test_csv)\n",
    "    print(f\"[INFO] train={len(train_df)}, val={len(val_df)}, test={len(test_df)}\")\n",
    "\n",
    "    # Build global user/item index\n",
    "    all_users= pd.concat([train_df[\"user_id\"], val_df[\"user_id\"], test_df[\"user_id\"]]).unique()\n",
    "    all_items= pd.concat([train_df[\"item_id\"], val_df[\"item_id\"], test_df[\"item_id\"]]).unique()\n",
    "    user2idx= {u:i for i,u in enumerate(all_users)}\n",
    "    item2idx= {i:u for u,i in enumerate(all_items)}\n",
    "\n",
    "    # 2) Build PyTorch datasets\n",
    "    train_ds= MindInteractionDataset(train_df, user2idx, item2idx)\n",
    "    val_ds= MindInteractionDataset(val_df, user2idx, item2idx)\n",
    "    test_ds= MindInteractionDataset(test_df, user2idx, item2idx)\n",
    "\n",
    "    train_loader= DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader  = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # 3) Build NeuMF\n",
    "    model= NeuMF(len(user2idx), len(item2idx), emb_dim, mlp_hidden).to(device)\n",
    "    optimizer= optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn= nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # 4) Train\n",
    "    for ep in range(epochs):\n",
    "        tr_loss= train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "        vl_loss= eval_one_epoch(model, val_loader,   loss_fn, device)\n",
    "        print(f\"Epoch {ep}: train_loss={tr_loss:.4f}, val_loss={vl_loss:.4f}\")\n",
    "\n",
    "    # 5) Load cox data => cox_map\n",
    "    cox_map= load_cox_data_and_survival(\n",
    "        cox_data_csv, cox_survival_csv, itemHourLog_csv\n",
    "    )\n",
    "\n",
    "    # 6) Evaluate with GRV => final\n",
    "    hr, ndcg, cov, newcov= evaluate_ranking(\n",
    "        model, test_df, user2idx, item2idx, cox_map,\n",
    "        gamma=gamma,\n",
    "        K=10,\n",
    "        device=device,\n",
    "        new_threshold=100\n",
    "    )\n",
    "    print(f\"[RESULT] HR@10={hr:.4f}, NDCG@10={ndcg:.4f}, coverage@10={cov}, new_item_coverage@10={newcov}\")"
   ],
   "id": "10182d5ece72057b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "neumf_experiment(\n",
    "    train_csv=\"train.csv\",\n",
    "    val_csv=\"dev.csv\",\n",
    "    test_csv=\"test.csv\",\n",
    "    cox_data_csv=\"./cox_output/cox_data.csv\",\n",
    "    cox_survival_csv=\"./cox_output/cox_survival.csv\",\n",
    "    itemHourLog_csv=\"./output/ItemHourLog.csv\",\n",
    "    gamma=0,\n",
    "    epochs=3,\n",
    "    batch_size=256,\n",
    "    emb_dim=8,\n",
    "    mlp_hidden=16,\n",
    "    use_cuda=True\n",
    ")"
   ],
   "id": "7ab4a8530a13f7a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "neumf_experiment(\n",
    "    train_csv=\"train.csv\",\n",
    "    val_csv=\"dev.csv\",\n",
    "    test_csv=\"test.csv\",\n",
    "    cox_data_csv=\"./cox_output/cox_data.csv\",\n",
    "    cox_survival_csv=\"./cox_output/cox_survival.csv\",\n",
    "    itemHourLog_csv=\"./output/ItemHourLog.csv\",\n",
    "    gamma=0.3,\n",
    "    epochs=3,\n",
    "    batch_size=256,\n",
    "    emb_dim=8,\n",
    "    mlp_hidden=16,\n",
    "    use_cuda=True\n",
    ")"
   ],
   "id": "a341adbfcd27c942"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.2) GRU4REC",
   "id": "189729d02c531cef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#######################################################\n",
    "# 1) Load GRV Data (Item Popularity Over Time)\n",
    "#######################################################\n",
    "\n",
    "def load_cox_data_and_survival(cox_data_csv, cox_survival_csv, itemHourLog_csv):\n",
    "    cox_df = pd.read_csv(cox_data_csv)\n",
    "    if \"T_i0\" in cox_df.columns:\n",
    "        t0_map = dict(zip(cox_df[\"item_id\"], cox_df[\"T_i0\"]))\n",
    "    else:\n",
    "        hour_df = pd.read_csv(itemHourLog_csv)\n",
    "        tmp = hour_df.groupby(\"item_id\")[\"hour_offset\"].min().reset_index()\n",
    "        t0_map = dict(zip(tmp[\"item_id\"], tmp[\"hour_offset\"]))\n",
    "\n",
    "    surv_df = pd.read_csv(cox_survival_csv)\n",
    "    grv_cols = [c for c in surv_df.columns if c.startswith(\"GRV_t\")]\n",
    "\n",
    "    def parse_off(col):\n",
    "        return int(col.split(\"t\")[-1])\n",
    "\n",
    "    item_grv = {}\n",
    "    for row in surv_df.itertuples(index=False):\n",
    "        it = getattr(row, \"item_id\")\n",
    "        d = {parse_off(c): getattr(row, c) for c in grv_cols}\n",
    "        item_grv[it] = d\n",
    "\n",
    "    cox_map = {it_id: {\"T_i0\": t0_map.get(it_id, 0), \"grv_map\": item_grv[it_id]} for it_id in item_grv}\n",
    "    return cox_map\n",
    "\n",
    "\n",
    "def get_grv(cox_map, item_id, current_hour, default_val=0.0):\n",
    "    if item_id not in cox_map:\n",
    "        return default_val\n",
    "    T0 = cox_map[item_id][\"T_i0\"]\n",
    "    offset = int(current_hour - T0)\n",
    "    if offset <= 0:\n",
    "        return 0.0\n",
    "    grv_map = cox_map[item_id][\"grv_map\"]\n",
    "    offsets = sorted(grv_map.keys())\n",
    "    offset = min(max(offset, offsets[0]), offsets[-1])\n",
    "    return grv_map.get(offset, default_val)\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# 2) GRU4Rec Model (Session-Based Recommendation)\n",
    "#######################################################\n",
    "\n",
    "class GRU4Rec(nn.Module):\n",
    "    def __init__(self, num_items, emb_dim=16, hidden_size=16, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.item_emb = nn.Embedding(num_items, emb_dim)\n",
    "        self.gru = nn.GRU(input_size=emb_dim, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_items)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def forward(self, session_seq):\n",
    "        embedded = self.item_emb(session_seq)\n",
    "        gru_out, _ = self.gru(embedded)\n",
    "        last_out = gru_out[:, -1, :]\n",
    "        last_out = self.dropout(last_out)\n",
    "        return self.fc(last_out)\n",
    "\n",
    "\n",
    "class SessionDataset(Dataset):\n",
    "    def __init__(self, df, item2idx, session_length=5):\n",
    "        self.sessions = []\n",
    "        grouped = df.groupby(\"user_id\")\n",
    "        for _, group in grouped:\n",
    "            group = group.sort_values(\"time\")\n",
    "            items = group[\"item_id\"].values\n",
    "            if len(items) < session_length:\n",
    "                continue\n",
    "            for i in range(len(items) - session_length + 1):\n",
    "                session_seq = items[i: i + session_length]\n",
    "                indices = [item2idx[it] for it in session_seq if it in item2idx]\n",
    "                if len(indices) == session_length:\n",
    "                    self.sessions.append(indices)\n",
    "        self.sessions = np.array(self.sessions)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sessions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        session = self.sessions[idx]\n",
    "        return torch.tensor(session[:-1], dtype=torch.long), torch.tensor(session[-1], dtype=torch.long)\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# 3) Training & Evaluation for GRU4Rec\n",
    "#######################################################\n",
    "\n",
    "def train_one_epoch_gru4rec(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, target in loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)\n",
    "        loss = loss_fn(logits, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def eval_one_epoch_gru4rec(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, target in loader:\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            logits = model(inputs)\n",
    "            loss = loss_fn(logits, target)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == target).sum().item()\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# 4) GRV-Based Ranking for GRU4Rec\n",
    "#######################################################\n",
    "\n",
    "def evaluate_gru4rec_ranking(model, test_df, item2idx, cox_map, gamma=0.3, K=10, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Evaluate GRU4Rec performance using HR@K, NDCG@K, Coverage@K, and New Item Coverage@K.\n",
    "    \"\"\"\n",
    "    test_df = test_df.copy()\n",
    "    test_df[\"time_hr\"] = (test_df[\"time\"] // 3600).astype(int)\n",
    "\n",
    "    grouped = test_df.groupby(\"user_id\")\n",
    "    coverage_items = set()\n",
    "    new_item_hits = 0  # Number of users who received at least one new item\n",
    "\n",
    "    hits_at_k = 0\n",
    "    ndcg_at_k = 0  # ✅ Track cumulative NDCG\n",
    "    total_positives = 0\n",
    "    all_users = list(grouped.groups.keys())\n",
    "    all_item_ids = list(item2idx.keys())\n",
    "\n",
    "    rng = np.random.default_rng(0)\n",
    "\n",
    "    for user_id in tqdm(all_users, desc=\"Evaluating GRU4Rec with GRV\"):\n",
    "        g = grouped.get_group(user_id)\n",
    "        t_hr = g[\"time_hr\"].min()\n",
    "        pos_items = g[g[\"label\"] == 1][\"item_id\"].unique()\n",
    "        candidate_items = np.unique(np.concatenate([pos_items, rng.choice(all_item_ids, size=50, replace=False)]))\n",
    "\n",
    "        valid_candidates = [(it, item2idx[it]) for it in candidate_items if it in item2idx]\n",
    "        if not valid_candidates:\n",
    "            continue\n",
    "\n",
    "        item_indices = [idx for _, idx in valid_candidates]\n",
    "        item_ids = [it for it, _ in valid_candidates]\n",
    "\n",
    "        # Compute model scores\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = torch.tensor(item_indices, dtype=torch.long, device=device).unsqueeze(0)\n",
    "            logits = model(inputs)\n",
    "        base_scores = logits.cpu().numpy().flatten()\n",
    "\n",
    "        # Compute final scores using GRV\n",
    "        final_scores = []\n",
    "        for i, (it, base_score) in enumerate(zip(item_ids, base_scores)):\n",
    "            grv_val = get_grv(cox_map, it, t_hr)\n",
    "            final_score = (1 - gamma) * base_score + gamma * grv_val\n",
    "            final_scores.append(final_score)\n",
    "\n",
    "        # Select top-K items\n",
    "        top_indices = np.argsort(-np.array(final_scores))[:K]\n",
    "        top_items = [item_ids[i] for i in top_indices]\n",
    "\n",
    "        # **Coverage Calculation**\n",
    "        coverage_items.update(top_items)\n",
    "\n",
    "        # **New Item Coverage Calculation**\n",
    "        new_items = set(it for it in top_items if it in cox_map and cox_map[it][\"T_i0\"] >= 100)\n",
    "        if new_items:\n",
    "            new_item_hits += 1  # Count users who received at least one new item\n",
    "\n",
    "        # **HR Calculation**\n",
    "        total_positives += len(pos_items)\n",
    "        hits = sum(1 for pos_it in pos_items if pos_it in top_items)\n",
    "        hits_at_k += hits\n",
    "\n",
    "        # **NDCG Calculation**\n",
    "        dcg = 0.0\n",
    "        for pos_it in pos_items:\n",
    "            if pos_it in top_items:\n",
    "                rank = np.where(np.array(top_items) == pos_it)[0][0] + 1\n",
    "                dcg += 1 / np.log2(rank + 1)  # ✅ Compute DCG\n",
    "\n",
    "        # Ideal DCG (iDCG) - best possible ranking\n",
    "        idcg = sum(1.0 / np.log2(i + 2) for i in range(len(pos_items))) if len(pos_items) > 0 else 0\n",
    "        ndcg_at_k += dcg / idcg if idcg > 0 else 0  # ✅ Normalize DCG\n",
    "\n",
    "    # **Normalize Metrics**\n",
    "    hr = hits_at_k / total_positives if total_positives > 0 else 0\n",
    "    ndcg = ndcg_at_k / len(all_users) if len(all_users) > 0 else 0  # ✅ Normalize by total users\n",
    "    coverage = len(coverage_items) / len(item2idx)  # ✅ Normalize by total items\n",
    "    new_item_coverage = new_item_hits / len(all_users)  # ✅ Normalize by total users\n",
    "\n",
    "    return hr, ndcg, coverage, new_item_coverage\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# 5) Main Experiment for GRU4Rec with GRV\n",
    "#######################################################\n",
    "\n",
    "def gru4rec(gamma_val):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_df, val_df, test_df = load_mind_data_with_neg(\"train.csv\", \"dev.csv\", \"test.csv\")\n",
    "    item2idx = {i: idx for idx, i in\n",
    "                enumerate(pd.concat([train_df[\"item_id\"], val_df[\"item_id\"], test_df[\"item_id\"]]).unique())}\n",
    "\n",
    "    train_ds = SessionDataset(train_df, item2idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "\n",
    "    model = GRU4Rec(len(item2idx)).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for ep in range(3):\n",
    "        train_one_epoch_gru4rec(model, train_loader, optimizer, loss_fn, device)\n",
    "\n",
    "    cox_map = load_cox_data_and_survival(\"./cox_output/cox_data.csv\", \"./cox_output/cox_survival.csv\",\n",
    "                                         \"./output/itemHourLog.csv\")\n",
    "\n",
    "    hr, ndcg, cov, newcov = evaluate_gru4rec_ranking(model, test_df, item2idx, cox_map, gamma=gamma_val, K=10, device=device)\n",
    " \n",
    "    print(f\"[RESULT] HR@10={hr:.4f}, NDCG@10={ndcg:.4f}, coverage@10={cov:.4f}, new_item_coverage@10={newcov:.4f}\")"
   ],
   "id": "61af7b0d02b3180f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "gru4rec(0)",
   "id": "5f0a60fbd3744053"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "gru4rec(0.3)",
   "id": "2273977e20443036"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.3) Tisas",
   "id": "9fd8dfdbb18ae568"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#######################################################\n",
    "# 1) Load GRV Data (Item Popularity Over Time)\n",
    "#######################################################\n",
    "\n",
    "def load_cox_data_and_survival(cox_data_csv, cox_survival_csv, itemHourLog_csv):\n",
    "    cox_df = pd.read_csv(cox_data_csv)\n",
    "    if \"T_i0\" in cox_df.columns:\n",
    "        t0_map = dict(zip(cox_df[\"item_id\"], cox_df[\"T_i0\"]))\n",
    "    else:\n",
    "        hour_df = pd.read_csv(itemHourLog_csv)\n",
    "        tmp = hour_df.groupby(\"item_id\")[\"hour_offset\"].min().reset_index()\n",
    "        t0_map = dict(zip(tmp[\"item_id\"], tmp[\"hour_offset\"]))\n",
    "\n",
    "    surv_df = pd.read_csv(cox_survival_csv)\n",
    "    grv_cols = [c for c in surv_df.columns if c.startswith(\"GRV_t\")]\n",
    "\n",
    "    def parse_off(col):\n",
    "        return int(col.split(\"t\")[-1])\n",
    "\n",
    "    item_grv = {}\n",
    "    for row in surv_df.itertuples(index=False):\n",
    "        it = getattr(row, \"item_id\")\n",
    "        d = {parse_off(c): getattr(row, c) for c in grv_cols}\n",
    "        item_grv[it] = d\n",
    "\n",
    "    cox_map = {it_id: {\"T_i0\": t0_map.get(it_id, 0), \"grv_map\": item_grv[it_id]} for it_id in item_grv}\n",
    "    return cox_map\n",
    "\n",
    "\n",
    "def get_grv(cox_map, item_id, current_hour, default_val=0.0):\n",
    "    if item_id not in cox_map:\n",
    "        return default_val\n",
    "    T0 = cox_map[item_id][\"T_i0\"]\n",
    "    offset = int(current_hour - T0)\n",
    "    if offset <= 0:\n",
    "        return 0.0\n",
    "    grv_map = cox_map[item_id][\"grv_map\"]\n",
    "    offsets = sorted(grv_map.keys())\n",
    "    offset = min(max(offset, offsets[0]), offsets[-1])\n",
    "    return grv_map.get(offset, default_val)\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# 2) TiSASRec Model (Time-Aware Self-Attention)\n",
    "#######################################################\n",
    "\n",
    "class TiSASRec(nn.Module):\n",
    "    def __init__(self, num_items, emb_dim=16, num_heads=2, num_layers=2, max_seq_len=50, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len  # Ensure max length is set\n",
    "        self.item_emb = nn.Embedding(num_items, emb_dim)\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, emb_dim)\n",
    "        self.time_emb = nn.Linear(1, emb_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=num_heads, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(emb_dim, num_items)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def forward(self, item_seq, time_seq):\n",
    "        batch_size, seq_len = item_seq.shape\n",
    "        if seq_len > self.max_seq_len:\n",
    "            item_seq = item_seq[:, -self.max_seq_len:]  # Truncate to max_seq_len\n",
    "            time_seq = time_seq[:, -self.max_seq_len:]  # Truncate to match\n",
    "\n",
    "        item_embeddings = self.item_emb(item_seq)\n",
    "        pos_indices = torch.arange(item_seq.shape[1], device=item_seq.device).unsqueeze(0).expand_as(item_seq)\n",
    "        pos_embeddings = self.pos_emb(pos_indices)\n",
    "        time_embeddings = self.time_emb(time_seq.unsqueeze(-1))\n",
    "\n",
    "        seq_embeddings = item_embeddings + pos_embeddings + time_embeddings\n",
    "        seq_embeddings = self.encoder(seq_embeddings)\n",
    "        last_output = seq_embeddings[:, -1, :]\n",
    "        return self.fc(self.dropout(last_output))\n",
    "\n",
    "\n",
    "class TimeAwareSessionDataset(Dataset):\n",
    "    def __init__(self, df, item2idx, session_length=5):\n",
    "        self.sessions = []\n",
    "        self.time_diffs = []\n",
    "        grouped = df.groupby(\"user_id\")\n",
    "        for _, group in grouped:\n",
    "            group = group.sort_values(\"time\")\n",
    "            items = group[\"item_id\"].values\n",
    "            times = group[\"time\"].values\n",
    "            if len(items) < session_length:\n",
    "                continue\n",
    "            for i in range(len(items) - session_length + 1):\n",
    "                session_seq = items[i: i + session_length]\n",
    "                time_seq = times[i: i + session_length] - times[i]\n",
    "                indices = [item2idx[it] for it in session_seq if it in item2idx]\n",
    "                if len(indices) == session_length:\n",
    "                    self.sessions.append(indices)\n",
    "                    self.time_diffs.append(time_seq)\n",
    "        self.sessions = np.array(self.sessions)\n",
    "        self.time_diffs = np.array(self.time_diffs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sessions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        session = self.sessions[idx]\n",
    "        time_diff = self.time_diffs[idx]\n",
    "        return torch.tensor(session[:-1], dtype=torch.long), torch.tensor(time_diff[:-1],\n",
    "                                                                          dtype=torch.float), torch.tensor(session[-1],\n",
    "                                                                                                           dtype=torch.long)\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# 3) Training TiSASRec\n",
    "#######################################################\n",
    "\n",
    "def train_one_epoch_tisasrec(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for item_seq, time_seq, target in loader:\n",
    "        item_seq, time_seq, target = item_seq.to(device), time_seq.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(item_seq, time_seq)\n",
    "        loss = loss_fn(logits, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * item_seq.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# 4) GRV-Based Ranking for TiSASRec\n",
    "#######################################################\n",
    "\n",
    "def evaluate_tisasrec_ranking(model, test_df, item2idx, cox_map, gamma=0.3, K=10, device=\"cpu\"):\n",
    "    test_df = test_df.copy()\n",
    "    test_df[\"time_hr\"] = (test_df[\"time\"] // 3600).astype(int)\n",
    "    grouped = test_df.groupby(\"user_id\")\n",
    "    coverage_items, newcov = set(), 0  # `newcov` starts as a float\n",
    "    hits_at_k, ndcg_at_k, total_positives = 0, 0, 0\n",
    "    all_users = list(grouped.groups.keys())\n",
    "    all_item_ids = test_df[\"item_id\"].unique()\n",
    "    rng = np.random.default_rng(0)\n",
    "\n",
    "    for user_id in tqdm(all_users, desc=\"Evaluating TiSASRec with GRV\"):\n",
    "        g = grouped.get_group(user_id)\n",
    "        t_hr = g[\"time_hr\"].min()\n",
    "        pos_items = g[g[\"label\"] == 1][\"item_id\"].unique()\n",
    "        candidate_items = np.unique(np.concatenate([pos_items, rng.choice(all_item_ids, size=50, replace=False)]))\n",
    "\n",
    "        valid_items = [item2idx[it] for it in candidate_items if it in item2idx]\n",
    "        if not valid_items:\n",
    "            continue\n",
    "        if len(valid_items) > model.max_seq_len:\n",
    "            valid_items = valid_items[-model.max_seq_len:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = torch.tensor(valid_items, dtype=torch.long, device=device).unsqueeze(0)\n",
    "            time_inputs = torch.zeros_like(inputs, dtype=torch.float)\n",
    "            logits = model(inputs, time_inputs)\n",
    "            base_scores = logits.cpu().numpy().flatten()\n",
    "\n",
    "        final_scores = [(1 - gamma) * base_score + gamma * get_grv(cox_map, it, t_hr) for it, base_score in\n",
    "                        zip(candidate_items, base_scores)]\n",
    "        top_items = candidate_items[np.argsort(-np.array(final_scores))[:K]]\n",
    "\n",
    "        # ✅ Normalize Coverage\n",
    "        coverage_items.update(top_items)\n",
    "\n",
    "        # ✅ Normalize New Item Coverage\n",
    "        new_item_count = sum(1 for it in top_items if it in cox_map and cox_map[it][\"T_i0\"] >= 100)\n",
    "        newcov += new_item_count / K  # Proportion of new items recommended\n",
    "\n",
    "        total_positives += len(pos_items)\n",
    "        hits = sum(1 for pos_it in pos_items if pos_it in top_items)\n",
    "        hits_at_k += hits\n",
    "        ndcg_at_k += sum(\n",
    "            1.0 / np.log2(np.where(top_items == pos_it)[0][0] + 2) for pos_it in pos_items if pos_it in top_items)\n",
    "\n",
    "    hr = hits_at_k / total_positives if total_positives > 0 else 0\n",
    "    ndcg = ndcg_at_k / len(all_users) if len(all_users) > 0 else 0\n",
    "    newcov /= len(all_users)  # Normalize new coverage by total users\n",
    "    coverage = len(coverage_items) / len(item2idx)  # Normalize coverage by total items\n",
    "\n",
    "    return hr, ndcg, coverage, newcov  # ✅ Coverage is now a proportion\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# 5) Main Experiment for TiSASRec with GRV\n",
    "#######################################################\n",
    "\n",
    "def tisas4rec(gamma):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load data using the correct function\n",
    "    train_df, val_df, test_df = load_mind_data_with_neg(\"train.csv\", \"dev.csv\", \"test.csv\")\n",
    "\n",
    "    # Debug: Print column names for verification\n",
    "    print(f\"[DEBUG] Columns in train.csv: {train_df.columns}\")\n",
    "\n",
    "    # Ensure 'item_id' column exists\n",
    "    if 'item_id' not in train_df.columns:\n",
    "        raise KeyError(\"Column 'item_id' is missing from train.csv. Check the column names.\")\n",
    "\n",
    "    # Build item2idx mapping\n",
    "    all_items = pd.concat([train_df[\"item_id\"], val_df[\"item_id\"], test_df[\"item_id\"]]).unique()\n",
    "    item2idx = {i: idx for idx, i in enumerate(all_items)}\n",
    "\n",
    "    # Load GRV data\n",
    "    cox_map = load_cox_data_and_survival(\n",
    "        \"cox_output/cox_data.csv\",\n",
    "        \"cox_output/cox_survival.csv\",\n",
    "        \"output/ItemHourLog.csv\"\n",
    "    )\n",
    "\n",
    "    # Build training dataset and loader\n",
    "    train_ds = TimeAwareSessionDataset(train_df, item2idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "\n",
    "    # Define model\n",
    "    model = TiSASRec(len(item2idx)).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train the model\n",
    "    for ep in range(3):\n",
    "        train_one_epoch_tisasrec(model, train_loader, optimizer, loss_fn, device)\n",
    "\n",
    "    # **Evaluate with GRV-based ranking**\n",
    "    hr, ndcg, cov, newcov = evaluate_tisasrec_ranking(\n",
    "        model, test_df, item2idx, cox_map, gamma=gamma, K=10, device=device\n",
    "    )\n",
    "\n",
    "    print(f\"[RESULT] HR@10={hr:.4f}, NDCG@10={ndcg:.4f}, coverage@10={cov}, new_item_coverage@10={newcov}\")\n"
   ],
   "id": "873748f2118c8717"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tisas4rec(0)",
   "id": "d6a811e413de72c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tisas4rec(0.3)",
   "id": "69ae0274005df4b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
