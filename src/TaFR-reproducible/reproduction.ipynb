{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d4966640c37cf67",
   "metadata": {},
   "source": [
    "# Experiment Reproduction Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51574e31c16782",
   "metadata": {},
   "source": [
    "### Requirements install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting anyio==4.8.0 (from -r requirements.txt (line 1))\n",
      "  Using cached anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: argon2-cffi==23.1.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (23.1.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings==21.2.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (21.2.0)\n",
      "Requirement already satisfied: arrow==1.3.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (1.3.0)\n",
      "Collecting asttokens==3.0.0 (from -r requirements.txt (line 5))\n",
      "  Using cached asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: async-lru==2.0.4 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (2.0.4)\n",
      "Collecting attrs==25.1.0 (from -r requirements.txt (line 7))\n",
      "  Using cached attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: autograd==1.7.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (1.7.0)\n",
      "Requirement already satisfied: autograd-gamma==0.5.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (0.5.0)\n",
      "Collecting babel==2.16.0 (from -r requirements.txt (line 10))\n",
      "  Using cached babel-2.16.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: beautifulsoup4==4.12.3 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (4.12.3)\n",
      "Collecting bleach==6.2.0 (from -r requirements.txt (line 12))\n",
      "  Using cached bleach-6.2.0-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: Brotli==1.1.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (1.1.0)\n",
      "Collecting certifi==2024.12.14 (from -r requirements.txt (line 14))\n",
      "  Using cached certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: cffi==1.17.1 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (1.17.1)\n",
      "Collecting charset-normalizer==3.4.1 (from -r requirements.txt (line 16))\n",
      "  Using cached charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Requirement already satisfied: colorama==0.4.6 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 17)) (0.4.6)\n",
      "Requirement already satisfied: comm==0.2.2 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 18)) (0.2.2)\n",
      "Requirement already satisfied: contourpy==1.3.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 19)) (1.3.0)\n",
      "Requirement already satisfied: cycler==0.12.1 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 20)) (0.12.1)\n",
      "Collecting debugpy==1.8.12 (from -r requirements.txt (line 21))\n",
      "  Using cached debugpy-1.8.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: decorator==5.1.1 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 22)) (5.1.1)\n",
      "Requirement already satisfied: defusedxml==0.7.1 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 23)) (0.7.1)\n",
      "Requirement already satisfied: exceptiongroup==1.2.2 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 24)) (1.2.2)\n",
      "Collecting executing==2.2.0 (from -r requirements.txt (line 25))\n",
      "  Using cached executing-2.2.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting fastjsonschema==2.21.1 (from -r requirements.txt (line 26))\n",
      "  Using cached fastjsonschema-2.21.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting feather-format==0.4.1 (from -r requirements.txt (line 27))\n",
      "  Using cached feather_format-0.4.1-py3-none-any.whl\n",
      "Requirement already satisfied: filelock==3.16.1 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 28)) (3.16.1)\n",
      "Collecting fonttools==4.55.3 (from -r requirements.txt (line 29))\n",
      "  Using cached fonttools-4.55.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (165 kB)\n",
      "Requirement already satisfied: formulaic==1.1.1 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 30)) (1.1.1)\n",
      "Requirement already satisfied: fqdn==1.5.1 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 31)) (1.5.1)\n",
      "Collecting fsspec==2024.12.0 (from -r requirements.txt (line 32))\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting git-filter-repo==2.47.0 (from -r requirements.txt (line 33))\n",
      "  Using cached git_filter_repo-2.47.0-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: h11==0.14.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 34)) (0.14.0)\n",
      "Collecting h5py==3.12.1 (from -r requirements.txt (line 35))\n",
      "  Using cached h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.0.7 (from -r requirements.txt (line 36))\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting httpx==0.28.1 (from -r requirements.txt (line 37))\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: idna==3.10 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 38)) (3.10)\n",
      "Collecting importlib_metadata==8.6.1 (from -r requirements.txt (line 39))\n",
      "  Using cached importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting importlib_resources==6.5.2 (from -r requirements.txt (line 40))\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting inflate64==1.0.1 (from -r requirements.txt (line 41))\n",
      "  Using cached inflate64-1.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: interface-meta==1.3.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 42)) (1.3.0)\n",
      "Requirement already satisfied: ipykernel==6.29.5 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 43)) (6.29.5)\n",
      "Collecting ipython==8.18.1 (from -r requirements.txt (line 44))\n",
      "  Using cached ipython-8.18.1-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: ipywidgets==8.1.5 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 45)) (8.1.5)\n",
      "Requirement already satisfied: isoduration==20.11.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 46)) (20.11.0)\n",
      "Collecting jedi==0.19.2 (from -r requirements.txt (line 47))\n",
      "  Using cached jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting Jinja2==3.1.5 (from -r requirements.txt (line 48))\n",
      "  Using cached jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: joblib==1.4.2 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 49)) (1.4.2)\n",
      "Collecting json5==0.10.0 (from -r requirements.txt (line 50))\n",
      "  Using cached json5-0.10.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: jsonpointer==3.0.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 51)) (3.0.0)\n",
      "Requirement already satisfied: jsonschema==4.23.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 52)) (4.23.0)\n",
      "Requirement already satisfied: jsonschema-specifications==2024.10.1 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 53)) (2024.10.1)\n",
      "Requirement already satisfied: jupyter==1.1.1 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 54)) (1.1.1)\n",
      "Requirement already satisfied: jupyter-console==6.6.3 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 55)) (6.6.3)\n",
      "Collecting jupyter-events==0.11.0 (from -r requirements.txt (line 56))\n",
      "  Using cached jupyter_events-0.11.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: jupyter-lsp==2.2.5 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 57)) (2.2.5)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 58)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.7.2 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 59)) (5.7.2)\n",
      "Collecting jupyter_server==2.15.0 (from -r requirements.txt (line 60))\n",
      "  Using cached jupyter_server-2.15.0-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: jupyter_server_terminals==0.5.3 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 61)) (0.5.3)\n",
      "Collecting jupyterlab==4.3.4 (from -r requirements.txt (line 62))\n",
      "  Using cached jupyterlab-4.3.4-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: jupyterlab_pygments==0.3.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 63)) (0.3.0)\n",
      "Requirement already satisfied: jupyterlab_server==2.27.3 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 64)) (2.27.3)\n",
      "Requirement already satisfied: jupyterlab_widgets==3.0.13 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 65)) (3.0.13)\n",
      "Requirement already satisfied: kiwisolver==1.4.7 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 66)) (1.4.7)\n",
      "Requirement already satisfied: lifelines==0.30.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 67)) (0.30.0)\n",
      "Requirement already satisfied: llvmlite==0.43.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 68)) (0.43.0)\n",
      "Collecting MarkupSafe==3.0.2 (from -r requirements.txt (line 69))\n",
      "  Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting matplotlib==3.9.4 (from -r requirements.txt (line 70))\n",
      "  Using cached matplotlib-3.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 71)) (0.1.7)\n",
      "Collecting mistune==3.1.0 (from -r requirements.txt (line 72))\n",
      "  Using cached mistune-3.1.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: mpmath==1.3.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 73)) (1.3.0)\n",
      "Collecting multivolumefile==0.2.3 (from -r requirements.txt (line 74))\n",
      "  Using cached multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting nbclient==0.10.2 (from -r requirements.txt (line 75))\n",
      "  Using cached nbclient-0.10.2-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting nbconvert==7.16.5 (from -r requirements.txt (line 76))\n",
      "  Using cached nbconvert-7.16.5-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: nbformat==5.10.4 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 77)) (5.10.4)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 78)) (1.6.0)\n",
      "Collecting networkx==3.2.1 (from -r requirements.txt (line 79))\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting notebook==7.3.2 (from -r requirements.txt (line 80))\n",
      "  Using cached notebook-7.3.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: notebook_shim==0.2.4 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 81)) (0.2.4)\n",
      "Requirement already satisfied: numba==0.60.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 82)) (0.60.0)\n",
      "Collecting numpy==2.0.2 (from -r requirements.txt (line 83))\n",
      "  Using cached numpy-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: overrides==7.7.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 84)) (7.7.0)\n",
      "Requirement already satisfied: packaging==24.2 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 85)) (24.2)\n",
      "Requirement already satisfied: pandas==2.2.3 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 86)) (2.2.3)\n",
      "Collecting pandocfilters==1.5.1 (from -r requirements.txt (line 87))\n",
      "  Using cached pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: parso==0.8.4 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 88)) (0.8.4)\n",
      "Collecting pillow==11.1.0 (from -r requirements.txt (line 89))\n",
      "  Using cached pillow-11.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: platformdirs==4.3.6 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 90)) (4.3.6)\n",
      "Collecting prometheus_client==0.21.1 (from -r requirements.txt (line 91))\n",
      "  Using cached prometheus_client-0.21.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting prompt_toolkit==3.0.50 (from -r requirements.txt (line 92))\n",
      "  Using cached prompt_toolkit-3.0.50-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting psutil==6.1.1 (from -r requirements.txt (line 93))\n",
      "  Using cached psutil-6.1.1-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 94)) (0.2.3)\n",
      "Collecting py7zr==0.22.0 (from -r requirements.txt (line 95))\n",
      "  Using cached py7zr-0.22.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting pyarrow==18.1.0 (from -r requirements.txt (line 96))\n",
      "  Using cached pyarrow-18.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting pybcj==1.0.3 (from -r requirements.txt (line 97))\n",
      "  Using cached pybcj-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting pycox==0.3.0 (from -r requirements.txt (line 98))\n",
      "  Using cached pycox-0.3.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: pycparser==2.22 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 99)) (2.22)\n",
      "Collecting pycryptodomex==3.21.0 (from -r requirements.txt (line 100))\n",
      "  Using cached pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting Pygments==2.19.1 (from -r requirements.txt (line 101))\n",
      "  Using cached pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting pyparsing==3.2.1 (from -r requirements.txt (line 102))\n",
      "  Using cached pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting pyppmd==1.1.1 (from -r requirements.txt (line 103))\n",
      "  Using cached pyppmd-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting python-dateutil==2.9.0.post0 (from -r requirements.txt (line 104))\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting python-json-logger==3.2.1 (from -r requirements.txt (line 105))\n",
      "  Using cached python_json_logger-3.2.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting pytz==2024.2 (from -r requirements.txt (line 106))\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pywin32==308 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pywin32==308\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1862fb2b8f3e674f",
   "metadata": {},
   "source": [
    "### 1) Item Hour Log generation from ReChorus TOPK generated MIND dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9663c9d13250308e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] ./ReChorus_MIND_dataset/val.csv not found. Skipping val.\n",
      "[INFO] Processing train ...\n",
      "[INFO] Processing test ...\n",
      "[INFO] Wrote basic item-hour logs to ./item_hour_log/ItemHourLog_basic.csv.\n",
      "[INFO] Wrote vitality-based itemHourLog to ./item_hour_log/ItemHourLog.csv.\n"
     ]
    }
   ],
   "source": [
    "!python 1_item_hour_log_from_ReChorus.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157acba30abce593",
   "metadata": {},
   "source": [
    "### 2) GRV generation from COX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67834d21-37ad-4a12-8cbe-e377ca3c8cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lifelines in /opt/conda/lib/python3.11/site-packages (0.30.0)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.11/site-packages (from lifelines) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from lifelines) (1.13.1)\n",
      "Requirement already satisfied: pandas>=2.1 in /opt/conda/lib/python3.11/site-packages (from lifelines) (2.2.3)\n",
      "Requirement already satisfied: matplotlib>=3.0 in /opt/conda/lib/python3.11/site-packages (from lifelines) (3.9.2)\n",
      "Requirement already satisfied: autograd>=1.5 in /opt/conda/lib/python3.11/site-packages (from lifelines) (1.7.0)\n",
      "Requirement already satisfied: autograd-gamma>=0.3 in /opt/conda/lib/python3.11/site-packages (from lifelines) (0.5.0)\n",
      "Requirement already satisfied: formulaic>=0.2.2 in /opt/conda/lib/python3.11/site-packages (from lifelines) (1.1.1)\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from formulaic>=0.2.2->lifelines) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.11/site-packages (from formulaic>=0.2.2->lifelines) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.0 in /opt/conda/lib/python3.11/site-packages (from formulaic>=0.2.2->lifelines) (1.17.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=2.1->lifelines) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=2.1->lifelines) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install lifelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2965d9515ef79d8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T08:39:37.937406Z",
     "start_time": "2025-02-02T08:39:35.159751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: norm_delta = 9.29e-01, step_size = 0.9500, log_lik = -6363.85370, newton_decrement = 3.00e+02, seconds_since_start = 0.0\n",
      "Iteration 2: norm_delta = 5.74e-01, step_size = 0.9500, log_lik = -6052.11557, newton_decrement = 2.65e+01, seconds_since_start = 0.0\n",
      "Iteration 3: norm_delta = 1.28e+00, step_size = 0.9500, log_lik = -6017.60925, newton_decrement = 2.62e+01, seconds_since_start = 0.0\n",
      "Iteration 4: norm_delta = 2.05e+00, step_size = 0.9310, log_lik = -5983.49713, newton_decrement = 2.07e+01, seconds_since_start = 0.0\n",
      "Iteration 5: norm_delta = 2.22e+00, step_size = 0.9124, log_lik = -5958.10495, newton_decrement = 9.66e+00, seconds_since_start = 0.0\n",
      "Iteration 6: norm_delta = 1.42e+00, step_size = 0.8941, log_lik = -5947.06383, newton_decrement = 2.15e+00, seconds_since_start = 0.0\n",
      "Iteration 7: norm_delta = 4.70e-01, step_size = 0.8762, log_lik = -5944.79223, newton_decrement = 1.74e-01, seconds_since_start = 0.1\n",
      "Iteration 8: norm_delta = 2.53e-02, step_size = 1.0000, log_lik = -5944.61269, newton_decrement = 4.55e-04, seconds_since_start = 0.1\n",
      "Iteration 9: norm_delta = 6.61e-05, step_size = 1.0000, log_lik = -5944.61224, newton_decrement = 3.09e-09, seconds_since_start = 0.1\n",
      "Convergence success after 9 iterations.\n",
      "[INFO] Saved cox_data.csv => ./cox_output/cox_data.csv\n",
      "[INFO] Wrote survival => ./cox_output/cox_survival.csv\n"
     ]
    }
   ],
   "source": [
    "!python 2_COX_GRV.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36df66435f201268",
   "metadata": {},
   "source": [
    "### 3) Backbone Models applying GAMMA = 0 and GAMMA = 0.3, 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d87d32d39511577f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T08:53:59.989659Z",
     "start_time": "2025-02-02T08:53:56.664272Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from math import log2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd4a351f469f976",
   "metadata": {},
   "source": [
    "#### 3.1) NeuMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10182d5ece72057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# 1) Data loading with negative samples\n",
    "#######################################################\n",
    "\n",
    "def load_mind_data_with_neg(train_csv, val_csv, test_csv):\n",
    "    \"\"\"\n",
    "    We assume train.csv might have columns: user_id, item_id, time, neg_items, etc.\n",
    "    We will parse them:\n",
    "      - For each row => (user, item, label=1)\n",
    "      - For each item in 'neg_items' => (user, neg_item, label=0)\n",
    "    Return dataframes for train, val, test with [user_id, item_id, time, label].\n",
    "    \"\"\"\n",
    "    def parse_dataset(filename):\n",
    "        data_pos = []\n",
    "        data_neg = []\n",
    "        if not os.path.exists(filename):\n",
    "            return pd.DataFrame(columns=[\"user_id\",\"item_id\",\"time\",\"label\"])\n",
    "        df = pd.read_csv(filename, sep=\"\\t\")\n",
    "        # If there's no 'neg_items' col => no negative sampling\n",
    "        if \"neg_items\" not in df.columns:\n",
    "            # fallback => treat all as label=1?\n",
    "            df[\"label\"] = 1\n",
    "            return df[[\"user_id\",\"item_id\",\"time\",\"label\"]]\n",
    "        # parse neg_items\n",
    "        for row in df.itertuples(index=False):\n",
    "            user = getattr(row,\"user_id\")\n",
    "            item = getattr(row,\"item_id\")\n",
    "            tval = getattr(row,\"time\")\n",
    "            # label=1\n",
    "            data_pos.append((user,item,tval,1))\n",
    "            # read neg_items => string of format \"[7856, 8058, ...]\"\n",
    "            s = getattr(row,\"neg_items\")\n",
    "            # parse them\n",
    "            s = s.strip()\n",
    "            s = s.lstrip(\"[\").rstrip(\"]\")\n",
    "            if len(s)>0:\n",
    "                parts = s.split(\",\")\n",
    "                for neg_str in parts:\n",
    "                    neg_str=neg_str.strip()\n",
    "                    if neg_str:\n",
    "                        neg_id = int(neg_str)\n",
    "                        data_neg.append((user,neg_id,tval,0))\n",
    "        df_pos = pd.DataFrame(data_pos, columns=[\"user_id\",\"item_id\",\"time\",\"label\"])\n",
    "        df_neg = pd.DataFrame(data_neg, columns=[\"user_id\",\"item_id\",\"time\",\"label\"])\n",
    "        finaldf = pd.concat([df_pos, df_neg], ignore_index=True)\n",
    "        return finaldf\n",
    "\n",
    "    train_df = parse_dataset(train_csv)\n",
    "    val_df   = parse_dataset(val_csv)\n",
    "    test_df  = parse_dataset(test_csv)\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "class MindInteractionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Basic PyTorch dataset with pointwise (user, item, label).\n",
    "    \"\"\"\n",
    "    def __init__(self, df, user2idx, item2idx):\n",
    "        self.users = df[\"user_id\"].map(user2idx).values\n",
    "        self.items = df[\"item_id\"].map(item2idx).values\n",
    "        self.labels= df[\"label\"].values.astype(float)\n",
    "        self.times = df[\"time\"].values.astype(int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.users[idx],\n",
    "            self.items[idx],\n",
    "            self.labels[idx],\n",
    "            self.times[idx]\n",
    "        )\n",
    "\n",
    "#######################################################\n",
    "# 2) NeuMF model\n",
    "#######################################################\n",
    "\n",
    "class NeuMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_dim=8, mlp_hidden=16):\n",
    "        super().__init__()\n",
    "        self.user_emb_gmf = nn.Embedding(num_users, emb_dim)\n",
    "        self.item_emb_gmf = nn.Embedding(num_items, emb_dim)\n",
    "\n",
    "        self.user_emb_mlp = nn.Embedding(num_users, emb_dim)\n",
    "        self.item_emb_mlp = nn.Embedding(num_items, emb_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2*emb_dim, mlp_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden, mlp_hidden//2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.final = nn.Linear(emb_dim + mlp_hidden//2, 1)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.user_emb_gmf.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb_gmf.weight)\n",
    "        nn.init.xavier_uniform_(self.user_emb_mlp.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb_mlp.weight)\n",
    "\n",
    "    def forward(self, user_idx, item_idx):\n",
    "        u_gmf = self.user_emb_gmf(user_idx)\n",
    "        i_gmf = self.item_emb_gmf(item_idx)\n",
    "        gmf_out= u_gmf*i_gmf\n",
    "\n",
    "        u_mlp = self.user_emb_mlp(user_idx)\n",
    "        i_mlp = self.item_emb_mlp(item_idx)\n",
    "        mlp_in= torch.cat([u_mlp, i_mlp], dim=1)\n",
    "        mlp_out= self.mlp(mlp_in)\n",
    "\n",
    "        concat = torch.cat([gmf_out, mlp_out], dim=1)\n",
    "        logit  = self.final(concat)\n",
    "        return logit.view(-1)\n",
    "\n",
    "#######################################################\n",
    "# 3) Training loop\n",
    "#######################################################\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    for batch in loader:\n",
    "        users, items, labels, _times = batch\n",
    "        users = users.to(device)\n",
    "        items = items.to(device)\n",
    "        labels= labels.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(users, items)\n",
    "        loss = loss_fn(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+= loss.item()*len(labels)\n",
    "    return total_loss/len(loader.dataset)\n",
    "\n",
    "def eval_one_epoch(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss=0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            users, items, labels, _times = batch\n",
    "            users= users.to(device)\n",
    "            items= items.to(device)\n",
    "            labels= labels.float().to(device)\n",
    "            preds= model(users, items)\n",
    "            loss= loss_fn(preds, labels)\n",
    "            total_loss+= loss.item()*len(labels)\n",
    "    return total_loss/len(loader.dataset)\n",
    "\n",
    "#######################################################\n",
    "# 4) Combine with GRV, Evaluate Coverage/HR@10\n",
    "#######################################################\n",
    "\n",
    "def load_cox_data_and_survival(cox_data_csv, cox_survival_csv, itemHourLog_csv):\n",
    "    cox_df = pd.read_csv(cox_data_csv)\n",
    "    if \"T_i0\" in cox_df.columns:\n",
    "        t0_map = dict(zip(cox_df[\"item_id\"], cox_df[\"T_i0\"]))\n",
    "    else:\n",
    "        hour_df= pd.read_csv(itemHourLog_csv)\n",
    "        tmp= hour_df.groupby(\"item_id\")[\"hour_offset\"].min().reset_index()\n",
    "        t0_map= dict(zip(tmp[\"item_id\"], tmp[\"hour_offset\"]))\n",
    "\n",
    "    surv_df= pd.read_csv(cox_survival_csv)\n",
    "    grv_cols= [c for c in surv_df.columns if c.startswith(\"GRV_t\")]\n",
    "    def parse_off(col):\n",
    "        return int(col.split(\"t\")[-1])\n",
    "\n",
    "    item_grv={}\n",
    "    for row in surv_df.itertuples(index=False):\n",
    "        it= getattr(row,\"item_id\")\n",
    "        d= {}\n",
    "        for c in grv_cols:\n",
    "            val= getattr(row,c)\n",
    "            off= parse_off(c)\n",
    "            d[off]= val\n",
    "        item_grv[it]= d\n",
    "\n",
    "    cox_map={}\n",
    "    for it_id in item_grv:\n",
    "        T0= t0_map[it_id] if it_id in t0_map else 0\n",
    "        cox_map[it_id]={\n",
    "            \"T_i0\": T0,\n",
    "            \"grv_map\": item_grv[it_id]\n",
    "        }\n",
    "    return cox_map\n",
    "\n",
    "def get_grv(cox_map, item_id, current_hour, default_val=0.0):\n",
    "    if item_id not in cox_map:\n",
    "        return default_val\n",
    "    T0= cox_map[item_id][\"T_i0\"]\n",
    "    offset= int(current_hour - T0)\n",
    "    if offset<=0:\n",
    "        return 0.0\n",
    "    grv_map= cox_map[item_id][\"grv_map\"]\n",
    "    offsets= sorted(grv_map.keys())\n",
    "    if offset< offsets[0]:\n",
    "        offset= offsets[0]\n",
    "    if offset> offsets[-1]:\n",
    "        offset= offsets[-1]\n",
    "    return grv_map.get(offset, default_val)\n",
    "\n",
    "def evaluate_ranking(\n",
    "    model, df_test, user2idx, item2idx, cox_map,\n",
    "    gamma=0.0, K=10, device=torch.device(\"cpu\"), new_threshold=100\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate ranking performance using HR@K, NDCG@K, Coverage@K, and New Item Coverage@K.\n",
    "    \"\"\"\n",
    "    df_test = df_test.copy()\n",
    "    df_test[\"time_hr\"] = (df_test[\"time\"] // 3600).astype(int)\n",
    "\n",
    "    grouped = df_test.groupby(\"user_id\")\n",
    "    coverage_items = set()\n",
    "    new_item_hits = 0\n",
    "\n",
    "    hits_at_k = 0\n",
    "    ndcg_at_k = 0\n",
    "    total_positives = 0\n",
    "\n",
    "    all_users = list(grouped.groups.keys())\n",
    "    all_item_ids = list(item2idx.keys())\n",
    "\n",
    "    rng = np.random.default_rng(0)\n",
    "\n",
    "    for user_id in tqdm(all_users, desc=\"EvaluateRanking\"):\n",
    "        g = grouped.get_group(user_id)\n",
    "        t_hr = g[\"time_hr\"].min()  # Earliest request\n",
    "\n",
    "        # Build a candidate set: positive items + 50 random items\n",
    "        pos_items = g[g[\"label\"] == 1][\"item_id\"].unique()\n",
    "        candidate_items = np.unique(np.concatenate([pos_items, rng.choice(all_item_ids, size=50, replace=False)]))\n",
    "\n",
    "        # Convert candidate items to indices, filtering valid ones\n",
    "        valid_candidates = [(it, item2idx[it]) for it in candidate_items if it in item2idx]\n",
    "        if not valid_candidates:\n",
    "            continue\n",
    "\n",
    "        item_indices = [idx for _, idx in valid_candidates]\n",
    "        item_ids = [it for it, _ in valid_candidates]\n",
    "\n",
    "        # Compute model scores\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            user_tensor = torch.tensor([user2idx[user_id]] * len(item_indices), dtype=torch.long, device=device)\n",
    "            item_tensor = torch.tensor(item_indices, dtype=torch.long, device=device)\n",
    "            preds = model(user_tensor, item_tensor)\n",
    "        base_scores = preds.cpu().numpy().flatten()\n",
    "\n",
    "        # Compute final scores using GRV\n",
    "        final_scores = []\n",
    "        for i, (it, base_score) in enumerate(zip(item_ids, base_scores)):\n",
    "            grv_val = get_grv(cox_map, it, t_hr)\n",
    "            final_score = (1 - gamma) * base_score + gamma * grv_val\n",
    "            final_scores.append(final_score)\n",
    "\n",
    "        # Select top-K items\n",
    "        top_indices = np.argsort(-np.array(final_scores))[:K]\n",
    "        top_items = [item_ids[i] for i in top_indices]\n",
    "\n",
    "        # **Coverage Calculation**\n",
    "        coverage_items.update(top_items)\n",
    "\n",
    "        # **New Item Coverage Calculation (Fixed)**\n",
    "        new_items = set(it for it in top_items if it in cox_map and cox_map[it][\"T_i0\"] >= new_threshold)\n",
    "        if new_items:\n",
    "            new_item_hits += 1  # Count users who received at least one new item\n",
    "\n",
    "        # **HR & NDCG Calculation**\n",
    "        total_positives += len(pos_items)\n",
    "        hits = sum(1 for pos_it in pos_items if pos_it in top_items)\n",
    "        hits_at_k += hits\n",
    "\n",
    "        # Compute NDCG\n",
    "        dcg = sum(1.0 / log2(np.where(np.array(top_items) == pos_it)[0][0] + 2) for pos_it in pos_items if pos_it in top_items)\n",
    "        idcg = sum(1.0 / log2(i + 2) for i in range(len(pos_items))) if len(pos_items) > 0 else 0\n",
    "        ndcg_at_k += dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "    # **Normalize Metrics**\n",
    "    hr = hits_at_k / total_positives if total_positives > 0 else 0\n",
    "    ndcg = ndcg_at_k / len(all_users) if len(all_users) > 0 else 0\n",
    "    coverage = len(coverage_items) / len(item2idx)  #  Normalize by total items\n",
    "    new_item_coverage = new_item_hits / len(all_users)  #  Normalize by total users\n",
    "\n",
    "    return hr, ndcg, coverage, new_item_coverage\n",
    "\n",
    "#######################################################\n",
    "# 5) Main experiment\n",
    "#######################################################\n",
    "def neumf_experiment(\n",
    "    train_csv=\"./ReChorus_MIND_dataset/train.csv\",\n",
    "    val_csv=\"./ReChorus_MIND_dataset/val.csv\",\n",
    "    test_csv=\"./ReChorus_MIND_dataset/test.csv\",\n",
    "    cox_data_csv=\"./cox_output/cox_data.csv\",\n",
    "    cox_survival_csv=\"./cox_output/cox_survival.csv\",\n",
    "    itemHourLog_csv=\"./output/ItemHourLog.csv\",\n",
    "    gamma=0.0,\n",
    "    epochs=3,\n",
    "    batch_size=256,\n",
    "    emb_dim=8,\n",
    "    mlp_hidden=16,\n",
    "    use_cuda=True\n",
    "):\n",
    "\n",
    "    device= torch.device(\"cuda:0\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] device={device}\")\n",
    "\n",
    "    # 1) Load data with negative sampling\n",
    "    train_df, val_df, test_df= load_mind_data_with_neg(train_csv, val_csv, test_csv)\n",
    "    print(f\"[INFO] train={len(train_df)}, val={len(val_df)}, test={len(test_df)}\")\n",
    "\n",
    "    # Build global user/item index\n",
    "    all_users= pd.concat([train_df[\"user_id\"], val_df[\"user_id\"], test_df[\"user_id\"]]).unique()\n",
    "    all_items= pd.concat([train_df[\"item_id\"], val_df[\"item_id\"], test_df[\"item_id\"]]).unique()\n",
    "    user2idx= {u:i for i,u in enumerate(all_users)}\n",
    "    item2idx= {i:u for u,i in enumerate(all_items)}\n",
    "\n",
    "    # 2) Build PyTorch datasets\n",
    "    train_ds= MindInteractionDataset(train_df, user2idx, item2idx)\n",
    "    val_ds= MindInteractionDataset(val_df, user2idx, item2idx)\n",
    "    test_ds= MindInteractionDataset(test_df, user2idx, item2idx)\n",
    "\n",
    "    train_loader= DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader  = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # 3) Build NeuMF\n",
    "    model= NeuMF(len(user2idx), len(item2idx), emb_dim, mlp_hidden).to(device)\n",
    "    optimizer= optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn= nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # 4) Train\n",
    "    for ep in range(epochs):\n",
    "        tr_loss= train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "        vl_loss= eval_one_epoch(model, val_loader,   loss_fn, device)\n",
    "        print(f\"Epoch {ep}: train_loss={tr_loss:.4f}, val_loss={vl_loss:.4f}\")\n",
    "\n",
    "    # 5) Load cox data => cox_map\n",
    "    cox_map= load_cox_data_and_survival(\n",
    "        cox_data_csv, cox_survival_csv, itemHourLog_csv\n",
    "    )\n",
    "\n",
    "    # 6) Evaluate with GRV => final\n",
    "    hr, ndcg, cov, newcov= evaluate_ranking(\n",
    "        model, test_df, user2idx, item2idx, cox_map,\n",
    "        gamma=gamma,\n",
    "        K=10,\n",
    "        device=device,\n",
    "        new_threshold=100\n",
    "    )\n",
    "    print(f\"[RESULT] HR@10={hr:.4f}, NDCG@10={ndcg:.4f}, coverage@10={cov}, new_item_coverage@10={newcov}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11049801-9210-43f3-aca2-75d8af01fbc5",
   "metadata": {},
   "source": [
    "Gamma = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ab4a8530a13f7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] device=cuda:0\n",
      "[INFO] train=2480957, val=17976500, test=10479700\n",
      "Epoch 0: train_loss=0.0084, val_loss=13.5235\n",
      "Epoch 1: train_loss=0.0000, val_loss=17.4974\n",
      "Epoch 2: train_loss=0.0000, val_loss=19.0179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EvaluateRanking: 100%|██████████| 57900/57900 [01:21<00:00, 709.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] HR@10=0.2048, NDCG@10=0.1144, coverage@10=0.38030797946803546, new_item_coverage@10=0.9444559585492228\n"
     ]
    }
   ],
   "source": [
    "neumf_experiment(\n",
    "    train_csv=\"./ReChorus_MIND_dataset/train.csv\",\n",
    "    val_csv=\"./ReChorus_MIND_dataset/dev.csv\",\n",
    "    test_csv=\"./ReChorus_MIND_dataset/test.csv\",\n",
    "    cox_data_csv=\"./cox_output/cox_data.csv\",\n",
    "    cox_survival_csv=\"./cox_output/cox_survival.csv\",\n",
    "    itemHourLog_csv=\"./output/ItemHourLog.csv\",\n",
    "    gamma=0,\n",
    "    epochs=3,\n",
    "    batch_size=256,\n",
    "    emb_dim=8,\n",
    "    mlp_hidden=16,\n",
    "    use_cuda=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b03db33-20e5-42bc-a040-297cee491f9f",
   "metadata": {},
   "source": [
    "Gamma = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a341adbfcd27c942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] device=cuda:0\n",
      "[INFO] train=2480957, val=17976500, test=10479700\n",
      "Epoch 0: train_loss=0.0131, val_loss=12.1629\n",
      "Epoch 1: train_loss=0.0000, val_loss=16.3767\n",
      "Epoch 2: train_loss=0.0000, val_loss=18.3364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EvaluateRanking: 100%|██████████| 57900/57900 [01:22<00:00, 702.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] HR@10=0.2068, NDCG@10=0.1156, coverage@10=0.37984134391040597, new_item_coverage@10=0.946286701208981\n"
     ]
    }
   ],
   "source": [
    "neumf_experiment(\n",
    "    train_csv=\"./ReChorus_MIND_dataset/train.csv\",\n",
    "    val_csv=\"./ReChorus_MIND_dataset/dev.csv\",\n",
    "    test_csv=\"./ReChorus_MIND_dataset/test.csv\",\n",
    "    cox_data_csv=\"./cox_output/cox_data.csv\",\n",
    "    cox_survival_csv=\"./cox_output/cox_survival.csv\",\n",
    "    itemHourLog_csv=\"./output/ItemHourLog.csv\",\n",
    "    gamma=0.3,\n",
    "    epochs=3,\n",
    "    batch_size=256,\n",
    "    emb_dim=8,\n",
    "    mlp_hidden=16,\n",
    "    use_cuda=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189729d02c531cef",
   "metadata": {},
   "source": [
    "#### 3.2) GRU4REC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61af7b0d02b3180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# 1) Load GRV Data (Item Popularity Over Time)\n",
    "#######################################################\n",
    "\n",
    "def load_cox_data_and_survival(cox_data_csv, cox_survival_csv, itemHourLog_csv):\n",
    "    cox_df = pd.read_csv(cox_data_csv)\n",
    "    if \"T_i0\" in cox_df.columns:\n",
    "        t0_map = dict(zip(cox_df[\"item_id\"], cox_df[\"T_i0\"]))\n",
    "    else:\n",
    "        hour_df = pd.read_csv(itemHourLog_csv)\n",
    "        tmp = hour_df.groupby(\"item_id\")[\"hour_offset\"].min().reset_index()\n",
    "        t0_map = dict(zip(tmp[\"item_id\"], tmp[\"hour_offset\"]))\n",
    "\n",
    "    surv_df = pd.read_csv(cox_survival_csv)\n",
    "    grv_cols = [c for c in surv_df.columns if c.startswith(\"GRV_t\")]\n",
    "\n",
    "    def parse_off(col):\n",
    "        return int(col.split(\"t\")[-1])\n",
    "\n",
    "    item_grv = {}\n",
    "    for row in surv_df.itertuples(index=False):\n",
    "        it = getattr(row, \"item_id\")\n",
    "        d = {parse_off(c): getattr(row, c) for c in grv_cols}\n",
    "        item_grv[it] = d\n",
    "\n",
    "    cox_map = {it_id: {\"T_i0\": t0_map.get(it_id, 0), \"grv_map\": item_grv[it_id]} for it_id in item_grv}\n",
    "    return cox_map\n",
    "\n",
    "\n",
    "def get_grv(cox_map, item_id, current_hour, default_val=0.0):\n",
    "    if item_id not in cox_map:\n",
    "        return default_val\n",
    "    T0 = cox_map[item_id][\"T_i0\"]\n",
    "    offset = int(current_hour - T0)\n",
    "    if offset <= 0:\n",
    "        return 0.0\n",
    "    grv_map = cox_map[item_id][\"grv_map\"]\n",
    "    offsets = sorted(grv_map.keys())\n",
    "    offset = min(max(offset, offsets[0]), offsets[-1])\n",
    "    return grv_map.get(offset, default_val)\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# 2) GRU4Rec Model (Session-Based Recommendation)\n",
    "#######################################################\n",
    "\n",
    "class GRU4Rec(nn.Module):\n",
    "    def __init__(self, num_items, emb_dim=16, hidden_size=16, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.item_emb = nn.Embedding(num_items, emb_dim)\n",
    "        self.gru = nn.GRU(input_size=emb_dim, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_items)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def forward(self, session_seq):\n",
    "        embedded = self.item_emb(session_seq)\n",
    "        gru_out, _ = self.gru(embedded)\n",
    "        last_out = gru_out[:, -1, :]\n",
    "        last_out = self.dropout(last_out)\n",
    "        return self.fc(last_out)\n",
    "\n",
    "\n",
    "class SessionDataset(Dataset):\n",
    "    def __init__(self, df, item2idx, session_length=5):\n",
    "        self.sessions = []\n",
    "        grouped = df.groupby(\"user_id\")\n",
    "        for _, group in grouped:\n",
    "            group = group.sort_values(\"time\")\n",
    "            items = group[\"item_id\"].values\n",
    "            if len(items) < session_length:\n",
    "                continue\n",
    "            for i in range(len(items) - session_length + 1):\n",
    "                session_seq = items[i: i + session_length]\n",
    "                indices = [item2idx[it] for it in session_seq if it in item2idx]\n",
    "                if len(indices) == session_length:\n",
    "                    self.sessions.append(indices)\n",
    "        self.sessions = np.array(self.sessions)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sessions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        session = self.sessions[idx]\n",
    "        return torch.tensor(session[:-1], dtype=torch.long), torch.tensor(session[-1], dtype=torch.long)\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# 3) Training & Evaluation for GRU4Rec\n",
    "#######################################################\n",
    "\n",
    "def train_one_epoch_gru4rec(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, target in loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)\n",
    "        loss = loss_fn(logits, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def eval_one_epoch_gru4rec(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, target in loader:\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            logits = model(inputs)\n",
    "            loss = loss_fn(logits, target)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == target).sum().item()\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# 4) GRV-Based Ranking for GRU4Rec\n",
    "#######################################################\n",
    "\n",
    "def evaluate_gru4rec_ranking(model, test_df, item2idx, cox_map, gamma=0.3, K=10, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Evaluate GRU4Rec performance using HR@K, NDCG@K, Coverage@K, and New Item Coverage@K.\n",
    "    \"\"\"\n",
    "    test_df = test_df.copy()\n",
    "    test_df[\"time_hr\"] = (test_df[\"time\"] // 3600).astype(int)\n",
    "\n",
    "    grouped = test_df.groupby(\"user_id\")\n",
    "    coverage_items = set()\n",
    "    new_item_hits = 0  # Number of users who received at least one new item\n",
    "\n",
    "    hits_at_k = 0\n",
    "    ndcg_at_k = 0  #  Track cumulative NDCG\n",
    "    total_positives = 0\n",
    "    all_users = list(grouped.groups.keys())\n",
    "    all_item_ids = list(item2idx.keys())\n",
    "\n",
    "    rng = np.random.default_rng(0)\n",
    "\n",
    "    for user_id in tqdm(all_users, desc=\"Evaluating GRU4Rec with GRV\"):\n",
    "        g = grouped.get_group(user_id)\n",
    "        t_hr = g[\"time_hr\"].min()\n",
    "        pos_items = g[g[\"label\"] == 1][\"item_id\"].unique()\n",
    "        candidate_items = np.unique(np.concatenate([pos_items, rng.choice(all_item_ids, size=50, replace=False)]))\n",
    "\n",
    "        valid_candidates = [(it, item2idx[it]) for it in candidate_items if it in item2idx]\n",
    "        if not valid_candidates:\n",
    "            continue\n",
    "\n",
    "        item_indices = [idx for _, idx in valid_candidates]\n",
    "        item_ids = [it for it, _ in valid_candidates]\n",
    "\n",
    "        # Compute model scores\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = torch.tensor(item_indices, dtype=torch.long, device=device).unsqueeze(0)\n",
    "            logits = model(inputs)\n",
    "        base_scores = logits.cpu().numpy().flatten()\n",
    "\n",
    "        # Compute final scores using GRV\n",
    "        final_scores = []\n",
    "        for i, (it, base_score) in enumerate(zip(item_ids, base_scores)):\n",
    "            grv_val = get_grv(cox_map, it, t_hr)\n",
    "            final_score = (1 - gamma) * base_score + gamma * grv_val\n",
    "            final_scores.append(final_score)\n",
    "\n",
    "        # Select top-K items\n",
    "        top_indices = np.argsort(-np.array(final_scores))[:K]\n",
    "        top_items = [item_ids[i] for i in top_indices]\n",
    "\n",
    "        # **Coverage Calculation**\n",
    "        coverage_items.update(top_items)\n",
    "\n",
    "        # **New Item Coverage Calculation**\n",
    "        new_items = set(it for it in top_items if it in cox_map and cox_map[it][\"T_i0\"] >= 100)\n",
    "        if new_items:\n",
    "            new_item_hits += 1  # Count users who received at least one new item\n",
    "\n",
    "        # **HR Calculation**\n",
    "        total_positives += len(pos_items)\n",
    "        hits = sum(1 for pos_it in pos_items if pos_it in top_items)\n",
    "        hits_at_k += hits\n",
    "\n",
    "        # **NDCG Calculation**\n",
    "        dcg = 0.0\n",
    "        for pos_it in pos_items:\n",
    "            if pos_it in top_items:\n",
    "                rank = np.where(np.array(top_items) == pos_it)[0][0] + 1\n",
    "                dcg += 1 / np.log2(rank + 1)  #  Compute DCG\n",
    "\n",
    "        # Ideal DCG (iDCG) - best possible ranking\n",
    "        idcg = sum(1.0 / np.log2(i + 2) for i in range(len(pos_items))) if len(pos_items) > 0 else 0\n",
    "        ndcg_at_k += dcg / idcg if idcg > 0 else 0  #  Normalize DCG\n",
    "\n",
    "    # **Normalize Metrics**\n",
    "    hr = hits_at_k / total_positives if total_positives > 0 else 0\n",
    "    ndcg = ndcg_at_k / len(all_users) if len(all_users) > 0 else 0  #  Normalize by total users\n",
    "    coverage = len(coverage_items) / len(item2idx)  #  Normalize by total items\n",
    "    new_item_coverage = new_item_hits / len(all_users)  #  Normalize by total users\n",
    "\n",
    "    return hr, ndcg, coverage, new_item_coverage\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# 5) Main Experiment for GRU4Rec with GRV\n",
    "#######################################################\n",
    "\n",
    "def gru4rec(gamma_val):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_df, val_df, test_df = load_mind_data_with_neg(\"./ReChorus_MIND_dataset/train.csv\", \n",
    "                                                        \"./ReChorus_MIND_dataset/dev.csv\", \n",
    "                                                        \"./ReChorus_MIND_dataset/test.csv\")\n",
    "    item2idx = {i: idx for idx, i in\n",
    "                enumerate(pd.concat([train_df[\"item_id\"], val_df[\"item_id\"], test_df[\"item_id\"]]).unique())}\n",
    "\n",
    "    train_ds = SessionDataset(train_df, item2idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "\n",
    "    model = GRU4Rec(len(item2idx)).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for ep in range(3):\n",
    "        train_one_epoch_gru4rec(model, train_loader, optimizer, loss_fn, device)\n",
    "\n",
    "    cox_map = load_cox_data_and_survival(\"./cox_output/cox_data.csv\", \"./cox_output/cox_survival.csv\",\n",
    "                                         \"./item_hour_log/itemHourLog.csv\")\n",
    "\n",
    "    hr, ndcg, cov, newcov = evaluate_gru4rec_ranking(model, test_df, item2idx, cox_map, gamma=gamma_val, K=10, device=device)\n",
    " \n",
    "    print(f\"[RESULT] HR@10={hr:.4f}, NDCG@10={ndcg:.4f}, coverage@10={cov:.4f}, new_item_coverage@10={newcov:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f0a60fbd3744053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating GRU4Rec with GRV: 100%|██████████| 57900/57900 [01:21<00:00, 711.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] HR@10=0.2015, NDCG@10=0.1065, coverage@10=0.9998, new_item_coverage@10=0.9812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gru4rec(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2273977e20443036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating GRU4Rec with GRV: 100%|██████████| 57900/57900 [01:20<00:00, 719.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] HR@10=0.2112, NDCG@10=0.1140, coverage@10=0.9984, new_item_coverage@10=0.9829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gru4rec(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd8dfdbb18ae568",
   "metadata": {},
   "source": [
    "### 3.3) Tisas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "873748f2118c8717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# 1) Load GRV Data (Item Popularity Over Time)\n",
    "#######################################################\n",
    "\n",
    "def load_cox_data_and_survival(cox_data_csv, cox_survival_csv, itemHourLog_csv):\n",
    "    cox_df = pd.read_csv(cox_data_csv)\n",
    "    if \"T_i0\" in cox_df.columns:\n",
    "        t0_map = dict(zip(cox_df[\"item_id\"], cox_df[\"T_i0\"]))\n",
    "    else:\n",
    "        hour_df = pd.read_csv(itemHourLog_csv)\n",
    "        tmp = hour_df.groupby(\"item_id\")[\"hour_offset\"].min().reset_index()\n",
    "        t0_map = dict(zip(tmp[\"item_id\"], tmp[\"hour_offset\"]))\n",
    "\n",
    "    surv_df = pd.read_csv(cox_survival_csv)\n",
    "    grv_cols = [c for c in surv_df.columns if c.startswith(\"GRV_t\")]\n",
    "\n",
    "    def parse_off(col):\n",
    "        return int(col.split(\"t\")[-1])\n",
    "\n",
    "    item_grv = {}\n",
    "    for row in surv_df.itertuples(index=False):\n",
    "        it = getattr(row, \"item_id\")\n",
    "        d = {parse_off(c): getattr(row, c) for c in grv_cols}\n",
    "        item_grv[it] = d\n",
    "\n",
    "    cox_map = {it_id: {\"T_i0\": t0_map.get(it_id, 0), \"grv_map\": item_grv[it_id]} for it_id in item_grv}\n",
    "    return cox_map\n",
    "\n",
    "\n",
    "def get_grv(cox_map, item_id, current_hour, default_val=0.0):\n",
    "    if item_id not in cox_map:\n",
    "        return default_val\n",
    "    T0 = cox_map[item_id][\"T_i0\"]\n",
    "    offset = int(current_hour - T0)\n",
    "    if offset <= 0:\n",
    "        return 0.0\n",
    "    grv_map = cox_map[item_id][\"grv_map\"]\n",
    "    offsets = sorted(grv_map.keys())\n",
    "    offset = min(max(offset, offsets[0]), offsets[-1])\n",
    "    return grv_map.get(offset, default_val)\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# 2) TiSASRec Model (Time-Aware Self-Attention)\n",
    "#######################################################\n",
    "\n",
    "class TiSASRec(nn.Module):\n",
    "    def __init__(self, num_items, emb_dim=16, num_heads=2, num_layers=2, max_seq_len=50, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len  # Ensure max length is set\n",
    "        self.item_emb = nn.Embedding(num_items, emb_dim)\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, emb_dim)\n",
    "        self.time_emb = nn.Linear(1, emb_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=num_heads, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(emb_dim, num_items)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def forward(self, item_seq, time_seq):\n",
    "        batch_size, seq_len = item_seq.shape\n",
    "        if seq_len > self.max_seq_len:\n",
    "            item_seq = item_seq[:, -self.max_seq_len:]  # Truncate to max_seq_len\n",
    "            time_seq = time_seq[:, -self.max_seq_len:]  # Truncate to match\n",
    "\n",
    "        item_embeddings = self.item_emb(item_seq)\n",
    "        pos_indices = torch.arange(item_seq.shape[1], device=item_seq.device).unsqueeze(0).expand_as(item_seq)\n",
    "        pos_embeddings = self.pos_emb(pos_indices)\n",
    "        time_embeddings = self.time_emb(time_seq.unsqueeze(-1))\n",
    "\n",
    "        seq_embeddings = item_embeddings + pos_embeddings + time_embeddings\n",
    "        seq_embeddings = self.encoder(seq_embeddings)\n",
    "        last_output = seq_embeddings[:, -1, :]\n",
    "        return self.fc(self.dropout(last_output))\n",
    "\n",
    "\n",
    "class TimeAwareSessionDataset(Dataset):\n",
    "    def __init__(self, df, item2idx, session_length=5):\n",
    "        self.sessions = []\n",
    "        self.time_diffs = []\n",
    "        grouped = df.groupby(\"user_id\")\n",
    "        for _, group in grouped:\n",
    "            group = group.sort_values(\"time\")\n",
    "            items = group[\"item_id\"].values\n",
    "            times = group[\"time\"].values\n",
    "            if len(items) < session_length:\n",
    "                continue\n",
    "            for i in range(len(items) - session_length + 1):\n",
    "                session_seq = items[i: i + session_length]\n",
    "                time_seq = times[i: i + session_length] - times[i]\n",
    "                indices = [item2idx[it] for it in session_seq if it in item2idx]\n",
    "                if len(indices) == session_length:\n",
    "                    self.sessions.append(indices)\n",
    "                    self.time_diffs.append(time_seq)\n",
    "        self.sessions = np.array(self.sessions)\n",
    "        self.time_diffs = np.array(self.time_diffs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sessions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        session = self.sessions[idx]\n",
    "        time_diff = self.time_diffs[idx]\n",
    "        return torch.tensor(session[:-1], dtype=torch.long), torch.tensor(time_diff[:-1],\n",
    "                                                                          dtype=torch.float), torch.tensor(session[-1],\n",
    "                                                                                                           dtype=torch.long)\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# 3) Training TiSASRec\n",
    "#######################################################\n",
    "\n",
    "def train_one_epoch_tisasrec(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for item_seq, time_seq, target in loader:\n",
    "        item_seq, time_seq, target = item_seq.to(device), time_seq.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(item_seq, time_seq)\n",
    "        loss = loss_fn(logits, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * item_seq.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# 4) GRV-Based Ranking for TiSASRec\n",
    "#######################################################\n",
    "\n",
    "def evaluate_tisasrec_ranking(model, test_df, item2idx, cox_map, gamma=0.3, K=10, device=\"cpu\"):\n",
    "    test_df = test_df.copy()\n",
    "    test_df[\"time_hr\"] = (test_df[\"time\"] // 3600).astype(int)\n",
    "    grouped = test_df.groupby(\"user_id\")\n",
    "    coverage_items, newcov = set(), 0  # `newcov` starts as a float\n",
    "    hits_at_k, ndcg_at_k, total_positives = 0, 0, 0\n",
    "    all_users = list(grouped.groups.keys())\n",
    "    all_item_ids = test_df[\"item_id\"].unique()\n",
    "    rng = np.random.default_rng(0)\n",
    "\n",
    "    for user_id in tqdm(all_users, desc=\"Evaluating TiSASRec with GRV\"):\n",
    "        g = grouped.get_group(user_id)\n",
    "        t_hr = g[\"time_hr\"].min()\n",
    "        pos_items = g[g[\"label\"] == 1][\"item_id\"].unique()\n",
    "        candidate_items = np.unique(np.concatenate([pos_items, rng.choice(all_item_ids, size=50, replace=False)]))\n",
    "\n",
    "        valid_items = [item2idx[it] for it in candidate_items if it in item2idx]\n",
    "        if not valid_items:\n",
    "            continue\n",
    "        if len(valid_items) > model.max_seq_len:\n",
    "            valid_items = valid_items[-model.max_seq_len:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = torch.tensor(valid_items, dtype=torch.long, device=device).unsqueeze(0)\n",
    "            time_inputs = torch.zeros_like(inputs, dtype=torch.float)\n",
    "            logits = model(inputs, time_inputs)\n",
    "            base_scores = logits.cpu().numpy().flatten()\n",
    "\n",
    "        final_scores = [(1 - gamma) * base_score + gamma * get_grv(cox_map, it, t_hr) for it, base_score in\n",
    "                        zip(candidate_items, base_scores)]\n",
    "        top_items = candidate_items[np.argsort(-np.array(final_scores))[:K]]\n",
    "\n",
    "        #  Normalize Coverage\n",
    "        coverage_items.update(top_items)\n",
    "\n",
    "        #  Normalize New Item Coverage\n",
    "        new_item_count = sum(1 for it in top_items if it in cox_map and cox_map[it][\"T_i0\"] >= 100)\n",
    "        newcov += new_item_count / K  # Proportion of new items recommended\n",
    "\n",
    "        total_positives += len(pos_items)\n",
    "        hits = sum(1 for pos_it in pos_items if pos_it in top_items)\n",
    "        hits_at_k += hits\n",
    "        ndcg_at_k += sum(\n",
    "            1.0 / np.log2(np.where(top_items == pos_it)[0][0] + 2) for pos_it in pos_items if pos_it in top_items)\n",
    "\n",
    "    hr = hits_at_k / total_positives if total_positives > 0 else 0\n",
    "    ndcg = ndcg_at_k / len(all_users) if len(all_users) > 0 else 0\n",
    "    newcov /= len(all_users)  # Normalize new coverage by total users\n",
    "    coverage = len(coverage_items) / len(item2idx)  # Normalize coverage by total items\n",
    "\n",
    "    return hr, ndcg, coverage, newcov  #  Coverage is now a proportion\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# 5) Main Experiment for TiSASRec with GRV\n",
    "#######################################################\n",
    "\n",
    "def tisas4rec(gamma):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load data using the correct function\n",
    "    train_df, val_df, test_df = load_mind_data_with_neg(\"./ReChorus_MIND_dataset/train.csv\", \n",
    "                                                        \"./ReChorus_MIND_dataset/dev.csv\", \n",
    "                                                        \"./ReChorus_MIND_dataset/test.csv\")\n",
    "\n",
    "    # Debug: Print column names for verification\n",
    "    print(f\"[DEBUG] Columns in train.csv: {train_df.columns}\")\n",
    "\n",
    "    # Ensure 'item_id' column exists\n",
    "    if 'item_id' not in train_df.columns:\n",
    "        raise KeyError(\"Column 'item_id' is missing from train.csv. Check the column names.\")\n",
    "\n",
    "    # Build item2idx mapping\n",
    "    all_items = pd.concat([train_df[\"item_id\"], val_df[\"item_id\"], test_df[\"item_id\"]]).unique()\n",
    "    item2idx = {i: idx for idx, i in enumerate(all_items)}\n",
    "\n",
    "    # Load GRV data\n",
    "    cox_map = load_cox_data_and_survival(\n",
    "        \"cox_output/cox_data.csv\",\n",
    "        \"cox_output/cox_survival.csv\",\n",
    "        \"output/ItemHourLog.csv\"\n",
    "    )\n",
    "\n",
    "    # Build training dataset and loader\n",
    "    train_ds = TimeAwareSessionDataset(train_df, item2idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "\n",
    "    # Define model\n",
    "    model = TiSASRec(len(item2idx)).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train the model\n",
    "    for ep in range(3):\n",
    "        train_one_epoch_tisasrec(model, train_loader, optimizer, loss_fn, device)\n",
    "\n",
    "    # **Evaluate with GRV-based ranking**\n",
    "    hr, ndcg, cov, newcov = evaluate_tisasrec_ranking(\n",
    "        model, test_df, item2idx, cox_map, gamma=gamma, K=10, device=device\n",
    "    )\n",
    "\n",
    "    print(f\"[RESULT] HR@10={hr:.4f}, NDCG@10={ndcg:.4f}, coverage@10={cov}, new_item_coverage@10={newcov}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6a811e413de72c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Columns in train.csv: Index(['user_id', 'item_id', 'time', 'label'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating TiSASRec with GRV: 100%|██████████| 57900/57900 [01:52<00:00, 512.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] HR@10=0.1937, NDCG@10=0.1560, coverage@10=0.6488567428838078, new_item_coverage@10=0.337960276338501\n"
     ]
    }
   ],
   "source": [
    "tisas4rec(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69ae0274005df4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Columns in train.csv: Index(['user_id', 'item_id', 'time', 'label'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating TiSASRec with GRV: 100%|██████████| 57900/57900 [01:53<00:00, 508.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] HR@10=0.1937, NDCG@10=0.1560, coverage@10=0.6488567428838078, new_item_coverage@10=0.337960276338501\n"
     ]
    }
   ],
   "source": [
    "tisas4rec(0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
