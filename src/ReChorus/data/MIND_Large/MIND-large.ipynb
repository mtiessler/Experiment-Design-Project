{
 "cells": [
  {
   "cell_type": "code",
   "id": "e47cdd78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T18:28:25.030074Z",
     "start_time": "2025-01-25T18:28:25.019089Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import subprocess\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "from tqdm.notebook import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "import json"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "id": "c251b774",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T18:28:25.125074Z",
     "start_time": "2025-01-25T18:28:25.113075Z"
    }
   },
   "source": [
    "DATASET = 'MIND_large' \n",
    "RAW_PATH = os.path.join('..','..','..', '..', 'data', DATASET)\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "NEG_ITEMS = 99"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "id": "a441d7b0",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "1. Load interaction data and item metadata\n",
    "2. Filter out items with less than 5 interactions\n",
    "3. Calculate basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "id": "47ed3175",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T18:28:31.969608Z",
     "start_time": "2025-01-25T18:28:25.143075Z"
    }
   },
   "source": [
    "# Please download the training and validation set from https://msnews.github.io/\n",
    "# and copy MINDlarge.zip and MINDlarge_dev.zip to the *MIND_large* dir\n",
    "print('Unzip files...')\n",
    "f = zipfile.ZipFile(os.path.join(RAW_PATH,'MINDlarge.zip'),'r') \n",
    "os.makedirs(os.path.join(RAW_PATH,'train'),exist_ok=True)\n",
    "for file in f.namelist():\n",
    "    print(\"Extract %s\"%(file))\n",
    "    f.extract(file,os.path.join(RAW_PATH,'train'))\n",
    "f.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzip files...\n",
      "Extract entity_embedding.vec\n",
      "Extract news.tsv\n",
      "Extract relation_embedding.vec\n",
      "Extract behaviors.tsv\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "id": "8f3f22ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T18:28:39.306563Z",
     "start_time": "2025-01-25T18:28:31.971610Z"
    }
   },
   "source": [
    "f = zipfile.ZipFile(os.path.join(RAW_PATH,'MINDlarge_dev.zip'),'r') \n",
    "os.makedirs(os.path.join(RAW_PATH,'dev'),exist_ok=True)\n",
    "for file in f.namelist():\n",
    "    print(\"Extract %s\"%(file))\n",
    "    f.extract(file,os.path.join(RAW_PATH,'dev'))\n",
    "f.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract entity_embedding.vec\n",
      "Extract news.tsv\n",
      "Extract relation_embedding.vec\n",
      "Extract behaviors.tsv\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "id": "2bdf8ec2",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-01-25T18:38:05.572242Z",
     "start_time": "2025-01-25T18:28:39.308558Z"
    }
   },
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "chunk_size = 5000  # Increase chunk size to reduce iterations\n",
    "interactions = []\n",
    "user_freq, item_freq = dict(), dict()\n",
    "\n",
    "max_rows = 1000000  # Limit the number of rows to process\n",
    "\n",
    "for d in [os.path.join(RAW_PATH, 'train'), os.path.join(RAW_PATH, 'dev')]:\n",
    "    file = os.path.join(d, \"behaviors.tsv\")\n",
    "    \n",
    "    total_lines = min(sum(1 for _ in open(file)), max_rows)  # Adjust total lines for the progress bar\n",
    "    \n",
    "    with tqdm(total=total_lines, desc=f\"Processing {file}\", unit=\"lines\", mininterval=1.0) as pbar:\n",
    "        for chunk in pd.read_csv(file, sep=\"\\t\", header=None, chunksize=chunk_size, nrows=max_rows):\n",
    "            # Preprocess chunk impressions\n",
    "            for _, row in chunk.iterrows():\n",
    "                sid, uid, time, _, impressions = row\n",
    "                impressions = [imp.split(\"-\") for imp in impressions.split(\" \")]\n",
    "                \n",
    "                # Update interactions\n",
    "                interactions.extend([[sid, uid, time, iid, label] for iid, label in impressions])\n",
    "                \n",
    "                # Batch update frequencies for positive interactions\n",
    "                for iid, label in impressions:\n",
    "                    if int(label) == 1:\n",
    "                        user_freq[uid] = user_freq.get(uid, 0) + 1\n",
    "                        item_freq[iid] = item_freq.get(iid, 0) + 1\n",
    "\n",
    "            # Update progress bar for processed chunk size\n",
    "            pbar.update(len(chunk))\n",
    "\n",
    "# Sampling a fraction of interactions if needed\n",
    "SAMPLE_FRACTION = 0.01  # Process only 1% of data to reduce memory usage\n",
    "interactions = np.array(interactions)\n",
    "interactions = interactions[np.random.choice(len(interactions), int(len(interactions) * SAMPLE_FRACTION), replace=False)].tolist()\n",
    "\n",
    "# Filter to retain only the top MAX_USERS and MAX_ITEMS by frequency\n",
    "MAX_USERS = 1000  # Limit the number of users\n",
    "MAX_ITEMS = 300  # Limit the number of items\n",
    "\n",
    "# Retain only the top users and items based on frequency\n",
    "top_users = set(sorted(user_freq, key=user_freq.get, reverse=True)[:MAX_USERS])\n",
    "top_items = set(sorted(item_freq, key=item_freq.get, reverse=True)[:MAX_ITEMS])\n",
    "\n",
    "# Filter interactions to include only top users and items\n",
    "filtered_interactions = [\n",
    "    interaction for interaction in interactions\n",
    "    if interaction[1] in top_users and interaction[3] in top_items\n",
    "]\n",
    "\n",
    "# Update interactions and frequencies with filtered data\n",
    "interactions = filtered_interactions\n",
    "user_freq = {u: user_freq[u] for u in top_users}\n",
    "item_freq = {i: item_freq[i] for i in top_items}\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ..\\..\\..\\..\\data\\MIND_large\\train\\behaviors.tsv: 100%|██████████| 1000000/1000000 [01:16<00:00, 13000.33lines/s]\n",
      "Processing ..\\..\\..\\..\\data\\MIND_large\\dev\\behaviors.tsv: 100%|██████████| 1000000/1000000 [01:54<00:00, 8709.15lines/s]\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000001380384C880>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Max\\Experiment-Design-Project\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 790, in _clean_thread_parent_frames\n",
      "    active_threads = {thread.ident for thread in threading.enumerate()}\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "id": "e8127286",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T18:38:05.680365Z",
     "start_time": "2025-01-25T18:38:05.603415Z"
    }
   },
   "source": [
    "interactions_original = interactions.copy()"
   ],
   "outputs": [],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "id": "ef143018",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-01-25T18:38:05.711931Z",
     "start_time": "2025-01-25T18:38:05.681366Z"
    }
   },
   "source": [
    "# 5-core filtering with reduced threshold\n",
    "MIN_INTERACTIONS = 5  # Reduced from 5 to make it lightweight\n",
    "\n",
    "# Step 1: Initial filtering of users and items\n",
    "select_uid, select_iid = [], []\n",
    "for u in user_freq:\n",
    "    if user_freq[u] >= MIN_INTERACTIONS:\n",
    "        select_uid.append(u)\n",
    "for i in item_freq:\n",
    "    if item_freq[i] >= MIN_INTERACTIONS:\n",
    "        select_iid.append(i)\n",
    "\n",
    "print(\"User: %d/%d, Item: %d/%d\" % (len(select_uid), len(user_freq), len(select_iid), len(item_freq)))\n",
    "\n",
    "# Step 2: Iterative filtering to ensure all users/items meet the threshold\n",
    "while len(select_uid) < len(user_freq) or len(select_iid) < len(item_freq):\n",
    "    # Convert to sets for faster lookups\n",
    "    select_uid = set(select_uid)\n",
    "    select_iid = set(select_iid)\n",
    "\n",
    "    # Reset frequency counters and filtered interactions\n",
    "    user_freq, item_freq = dict(), dict()\n",
    "    interactions_5core = []\n",
    "\n",
    "    # Filter interactions based on current user and item sets\n",
    "    for line in tqdm(interactions):\n",
    "        uid, iid, label = line[1], line[3], line[-1]\n",
    "        if uid in select_uid and iid in select_iid:\n",
    "            interactions_5core.append(line)\n",
    "            if int(label) == 1:\n",
    "                user_freq[uid] = user_freq.get(uid, 0) + 1\n",
    "                item_freq[iid] = item_freq.get(iid, 0) + 1\n",
    "\n",
    "    # Update the interactions list\n",
    "    interactions = interactions_5core\n",
    "\n",
    "    # Recompute selected users and items meeting the threshold\n",
    "    select_uid, select_iid = [], []\n",
    "    for u in user_freq:\n",
    "        if user_freq[u] >= MIN_INTERACTIONS:\n",
    "            select_uid.append(u)\n",
    "    for i in item_freq:\n",
    "        if item_freq[i] >= MIN_INTERACTIONS:\n",
    "            select_iid.append(i)\n",
    "\n",
    "    print(\"User: %d/%d, Item: %d/%d\" % (len(select_uid), len(user_freq), len(select_iid), len(item_freq)))\n",
    "\n",
    "# Final summary of filtered interactions\n",
    "print(\"Selected Interactions: %d, Users: %d, Items: %d\" % (len(interactions), len(select_uid), len(select_iid)))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 1000/1000, Item: 300/300\n",
      "Selected Interactions: 3186, Users: 1000, Items: 300\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "id": "6fe36067",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T18:38:05.727961Z",
     "start_time": "2025-01-25T18:38:05.714441Z"
    }
   },
   "source": [
    "# exclude illegal interactions\n",
    "for i in range(len(interactions)):\n",
    "    if len(interactions[i])>5:\n",
    "        interactions[i] = interactions[i][:-1]"
   ],
   "outputs": [],
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "id": "791906a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T18:38:05.790342Z",
     "start_time": "2025-01-25T18:38:05.729962Z"
    }
   },
   "source": [
    "# Get timestamp\n",
    "format_t = '%m/%d/%Y %I:%M:%S %p'\n",
    "ts, time = [], []\n",
    "for i in tqdm(range(len(interactions))):\n",
    "    t = datetime.strptime(interactions[i][2],format_t)\n",
    "    ts.append(t)\n",
    "    time.append(t.timestamp())"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3186/3186 [00:00<00:00, 105663.51it/s]\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "id": "9fd33b05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T18:38:05.914155Z",
     "start_time": "2025-01-25T18:38:05.792343Z"
    }
   },
   "source": [
    "# Construct 5 core results with situation context\n",
    "interaction_df = pd.DataFrame(interactions,columns = [\"session_id\",\"user_id\",\"time_str\",\"news_id\",\"label\"])\n",
    "interaction_df['time'] = time\n",
    "interaction_df['timestamp'] = ts\n",
    "interaction_df['hour'] = interaction_df['timestamp'].apply(lambda x: x.hour)\n",
    "interaction_df['weekday'] = interaction_df['timestamp'].apply(lambda x: x.weekday())\n",
    "interaction_df['date'] = interaction_df['timestamp'].apply(lambda x: x.date())\n",
    "\n",
    "def get_time_range(hour): # according to the Britannica dictionary\n",
    "    # https://www.britannica.com/dictionary/eb/qa/parts-of-the-day-early-morning-late-morning-etc\n",
    "    if hour>=5 and hour<=8:\n",
    "        return 0\n",
    "    if hour>8 and hour<11:\n",
    "        return 1\n",
    "    if hour>=11 and hour<=12:\n",
    "        return 2\n",
    "    if hour>12 and hour<=15:\n",
    "        return 3\n",
    "    if hour>15 and hour<=17:\n",
    "        return 4\n",
    "    if hour>=18 and hour<=19:\n",
    "        return 5\n",
    "    if hour>19 and hour<=21:\n",
    "        return 6\n",
    "    if hour>21:\n",
    "        return 7\n",
    "    return 8 # 0-4 am\n",
    "\n",
    "interaction_df['period'] = interaction_df.hour.apply(lambda x: get_time_range(x))\n",
    "min_date = interaction_df.date.min()\n",
    "interaction_df['day'] = (interaction_df.date - min_date).apply(lambda x: x.days)"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "id": "52cf7136",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T18:38:05.960367Z",
     "start_time": "2025-01-25T18:38:05.916156Z"
    }
   },
   "source": [
    "# Save 2-core interactions\n",
    "interaction_df.to_csv(f\"interaction_{MIN_INTERACTIONS}core.csv\",index=False)"
   ],
   "outputs": [],
   "execution_count": 63
  },
  {
   "cell_type": "markdown",
   "id": "cee70fad",
   "metadata": {},
   "source": [
    "----\n",
    "# Prepare data for CTR & Reranking task\n",
    "\n",
    "1. Rename and organize all interaction features\n",
    "2. Split dataset into training, validation, and test; Save interaction files (same time indicates same impression)\n",
    "3. Organize item metadata"
   ]
  },
  {
   "cell_type": "code",
   "id": "eeb124c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T18:38:05.975074Z",
     "start_time": "2025-01-25T18:38:05.963540Z"
    }
   },
   "source": [
    "CTR_PATH='./MINDCTR/'\n",
    "os.makedirs(CTR_PATH,exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "id": "746ae500",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T18:38:06.037284Z",
     "start_time": "2025-01-25T18:38:05.977077Z"
    }
   },
   "source": [
    "# copy interaction file, rename and re-id all features\n",
    "interaction_ctr = interaction_df.copy()\n",
    "interaction_ctr.rename(columns={'hour':'c_hour_c','weekday':'c_weekday_c','period':'c_period_c','day':'c_day_f',\n",
    "                              'user_id':'original_user_id'},\n",
    "                     inplace=True)\n",
    "user2newid_ctr = dict(zip(sorted(interaction_ctr.original_user_id.unique()), \n",
    "                      range(1,interaction_ctr.original_user_id.nunique()+1)))\n",
    "interaction_ctr['user_id'] = interaction_ctr.original_user_id.apply(lambda x: user2newid_ctr[x])\n",
    "\n",
    "item2newid_ctr = dict(zip(sorted(interaction_ctr.news_id.unique()), \n",
    "                      range(1,interaction_ctr.news_id.nunique()+1)))\n",
    "interaction_ctr['item_id'] = interaction_ctr['news_id'].apply(lambda x: item2newid_ctr[x])\n",
    "interaction_ctr.sort_values(by=['user_id','time'],inplace=True)\n",
    "interaction_ctr = interaction_ctr.reset_index(drop=True)\n",
    "\n",
    "json.dump(user2newid_ctr,open(os.path.join(CTR_PATH,\"user2newid.json\"),'w'))\n",
    "json.dump(item2newid_ctr,open(os.path.join(CTR_PATH,\"item2newid.json\"),'w'))"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "id": "022c3d8b",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-01-25T18:38:06.052320Z",
     "start_time": "2025-01-25T18:38:06.038286Z"
    }
   },
   "source": [
    "# Count statistics\n",
    "for col in interaction_ctr.columns:\n",
    "    if col in ['user_id','item_id'] or col.startswith('c_'):\n",
    "        print(col, interaction_ctr[col].nunique())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_hour_c 24\n",
      "c_weekday_c 6\n",
      "c_period_c 9\n",
      "c_day_f 6\n",
      "user_id 912\n",
      "item_id 299\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "id": "5a65c78a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T18:38:06.083457Z",
     "start_time": "2025-01-25T18:38:06.054324Z"
    }
   },
   "source": [
    "# split training, validation, and test sets.\n",
    "split_time1 = 5\n",
    "train = interaction_ctr.loc[interaction_ctr.c_day_f<=split_time1].copy()\n",
    "val_test = interaction_ctr.loc[(interaction_ctr.c_day_f>split_time1)].copy()\n",
    "val_test.sort_values(by='time',inplace=True)\n",
    "sessionbyTime = []\n",
    "last_s = -1\n",
    "for s in val_test.session_id:\n",
    "    if s!=last_s:\n",
    "        sessionbyTime.append(s)\n",
    "        last_s = s\n",
    "val = val_test.loc[val_test.session_id.isin(sessionbyTime[:len(sessionbyTime)//2])].copy()\n",
    "test = val_test.loc[val_test.session_id.isin(sessionbyTime[len(sessionbyTime)//2:])].copy()\n",
    "\n",
    "# Delete user&item in validation&test sets that not exist in training set\n",
    "train_u, train_i = set(train.user_id.unique()), set(train.item_id.unique())\n",
    "val_sel = val.copy()  # Do not filter validation set\n",
    "test_sel = test.copy()  # Do not filter test set\n",
    "print(\"Train user: %d, item: %d\"%(len(train_u),len(train_i)))\n",
    "print(\"Validation user: %d, item:%d\"%(val_sel.user_id.nunique(),val_sel.item_id.nunique()))\n",
    "print(\"Test user: %d, item:%d\"%(test_sel.user_id.nunique(),test_sel.item_id.nunique()))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train user: 912, item: 299\n",
      "Validation user: 0, item:0\n",
      "Test user: 0, item:0\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "id": "85cf4bb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T18:38:06.115117Z",
     "start_time": "2025-01-25T18:38:06.084963Z"
    }
   },
   "source": [
    "# Save interaction data\n",
    "select_columns = ['user_id','item_id','time','label','c_hour_c','c_weekday_c','c_period_c','c_day_f']\n",
    "train[select_columns].to_csv(os.path.join(CTR_PATH,'train.csv'),sep=\"\\t\",index=False)\n",
    "val_sel[select_columns].to_csv(os.path.join(CTR_PATH,'dev.csv'),sep=\"\\t\",index=False)\n",
    "test_sel[select_columns].to_csv(os.path.join(CTR_PATH,'test.csv'),sep=\"\\t\",index=False)"
   ],
   "outputs": [],
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "id": "c97e990d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T18:38:07.537989Z",
     "start_time": "2025-01-25T18:38:06.116116Z"
    }
   },
   "source": [
    "# organize & save item metadata\n",
    "item_meta_train = pd.read_csv(os.path.join(RAW_PATH,'train',\"news.tsv\"),sep=\"\\t\",header=None)\n",
    "item_meta_train.columns = ['news_id','category','subcategory','title','abstract','url','title_entitiy','abstract_entity']\n",
    "item_select = item_meta_train.loc[item_meta_train.news_id.isin(interaction_ctr.news_id.unique())].copy()\n",
    "item_select['item_id'] = item_select.news_id.apply(lambda x: item2newid_ctr[x])\n",
    "category2id = dict(zip(sorted(item_select.category.unique()),range(1,item_select.category.nunique()+1)))\n",
    "subcategory2id = dict(zip(sorted(item_select.subcategory.unique()),range(1,item_select.subcategory.nunique()+1)))\n",
    "item_select['i_category_c'] = item_select['category'].apply(lambda x: category2id[x])\n",
    "item_select['i_subcategory_c'] = item_select['subcategory'].apply(lambda x: subcategory2id[x])\n",
    "item_select[['item_id','i_category_c','i_subcategory_c']].to_csv(\n",
    "    os.path.join(CTR_PATH,'item_meta.csv'),sep=\"\\t\",index=False)"
   ],
   "outputs": [],
   "execution_count": 69
  },
  {
   "cell_type": "markdown",
   "id": "59eaf159",
   "metadata": {},
   "source": [
    "# Prepare data for Top-k Recommendation Task\n",
    "1. Rename all interaction features\n",
    "2. Split dataset into training, validation, and test\n",
    "3. Re-assign IDs to user, item, and context; Save interaction files\n",
    "4. Organize item metadata"
   ]
  },
  {
   "cell_type": "code",
   "id": "56e4bbe5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T18:38:07.553179Z",
     "start_time": "2025-01-25T18:38:07.539990Z"
    }
   },
   "source": [
    "TOPK_PATH='./MINDTOPK/'\n",
    "os.makedirs(TOPK_PATH,exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "id": "a8aadb00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T18:38:07.584987Z",
     "start_time": "2025-01-25T18:38:07.555183Z"
    }
   },
   "source": [
    "interaction_df = pd.read_csv(\"interaction_5core.csv\")"
   ],
   "outputs": [],
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "id": "210be2bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T18:38:07.615255Z",
     "start_time": "2025-01-25T18:38:07.586992Z"
    }
   },
   "source": [
    "interaction_df.head(2)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   session_id  user_id               time_str  news_id  label          time  \\\n",
       "0      171474  U140470  11/13/2019 8:32:50 PM    N5287      0  1.573674e+09   \n",
       "1      708259   U96447  11/12/2019 8:10:46 PM  N122819      0  1.573586e+09   \n",
       "\n",
       "             timestamp  hour  weekday        date  period  day  \n",
       "0  2019-11-13 20:32:50    20        2  2019-11-13       6    4  \n",
       "1  2019-11-12 20:10:46    20        1  2019-11-12       6    3  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>time_str</th>\n",
       "      <th>news_id</th>\n",
       "      <th>label</th>\n",
       "      <th>time</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>date</th>\n",
       "      <th>period</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>171474</td>\n",
       "      <td>U140470</td>\n",
       "      <td>11/13/2019 8:32:50 PM</td>\n",
       "      <td>N5287</td>\n",
       "      <td>0</td>\n",
       "      <td>1.573674e+09</td>\n",
       "      <td>2019-11-13 20:32:50</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-11-13</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>708259</td>\n",
       "      <td>U96447</td>\n",
       "      <td>11/12/2019 8:10:46 PM</td>\n",
       "      <td>N122819</td>\n",
       "      <td>0</td>\n",
       "      <td>1.573586e+09</td>\n",
       "      <td>2019-11-12 20:10:46</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-11-12</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "id": "baf110fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T18:38:07.630940Z",
     "start_time": "2025-01-25T18:38:07.617255Z"
    }
   },
   "source": [
    "# copy & rename columns\n",
    "interaction_pos = interaction_df.loc[interaction_df.label==1].copy() # retain positive interactions\n",
    "interaction_pos.rename(columns={'hour':'c_hour_c','weekday':'c_weekday_c','period':'c_period_c','day':'c_day_f',\n",
    "                              'user_id':'original_user_id'}, inplace=True)"
   ],
   "outputs": [],
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "id": "2bf648ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T18:38:07.646557Z",
     "start_time": "2025-01-25T18:38:07.633028Z"
    }
   },
   "source": [
    "# split training, validation, and test sets.\n",
    "split_time1 = 5\n",
    "train = interaction_pos.loc[interaction_pos.c_day_f<=split_time1].copy()\n",
    "val_test = interaction_pos.loc[(interaction_pos.c_day_f>split_time1)].copy()\n",
    "val_test.sort_values(by='time',inplace=True)\n",
    "sessionbyTime = []\n",
    "last_s = -1\n",
    "for s in val_test.session_id:\n",
    "    if s!=last_s:\n",
    "        sessionbyTime.append(s)\n",
    "        last_s = s\n",
    "val = val_test.loc[val_test.session_id.isin(sessionbyTime[:len(sessionbyTime)//2])].copy()\n",
    "test = val_test.loc[val_test.session_id.isin(sessionbyTime[len(sessionbyTime)//2:])].copy()\n",
    "\n",
    "# Delete user&item in validation&test sets that not exist in training set\n",
    "train_u, train_i = set(train.original_user_id.unique()), set(train.news_id.unique())\n",
    "val_sel = val.loc[(val.original_user_id.isin(train_u))&(val.news_id.isin(train_i))].copy()\n",
    "test_sel = test.loc[(test.original_user_id.isin(train_u))&(test.news_id.isin(train_i))].copy()\n",
    "print(\"Train user: %d, item: %d\"%(len(train_u),len(train_i)))\n",
    "print(\"Validation user: %d, item:%d\"%(val_sel.original_user_id.nunique(),val_sel.news_id.nunique()))\n",
    "print(\"Test user: %d, item:%d\"%(test_sel.original_user_id.nunique(),test_sel.news_id.nunique()))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train user: 245, item: 172\n",
      "Validation user: 0, item:0\n",
      "Test user: 0, item:0\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "id": "d405fb74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T18:38:07.677233Z",
     "start_time": "2025-01-25T18:38:07.648565Z"
    }
   },
   "source": [
    "# Assign ids for users and items (to generate continous ids)\n",
    "all_df = pd.concat([train,val_sel,test_sel],axis=0)\n",
    "user2newid_topk = dict(zip(sorted(all_df.original_user_id.unique()), \n",
    "                      range(1,all_df.original_user_id.nunique()+1)))\n",
    " \n",
    "for df in [train,val_sel,test_sel]:\n",
    "    df['user_id'] = df.original_user_id.apply(lambda x: user2newid_topk[x])\n",
    "\n",
    "item2newid_topk = dict(zip(sorted(all_df.news_id.unique()), \n",
    "                      range(1,all_df.news_id.nunique()+1)))\n",
    "for df in [train,val_sel,test_sel]:\n",
    "    df['item_id'] = df['news_id'].apply(lambda x: item2newid_topk[x])\n",
    "\n",
    "all_df['user_id'] = all_df.original_user_id.apply(lambda x: user2newid_topk[x])\n",
    "all_df['item_id'] = all_df['news_id'].apply(lambda x: item2newid_topk[x])\n",
    "\n",
    "json.dump(user2newid_topk,open(os.path.join(TOPK_PATH,\"user2newid.json\"),'w'))\n",
    "json.dump(item2newid_topk,open(os.path.join(TOPK_PATH,\"item2newid.json\"),'w'))"
   ],
   "outputs": [],
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "id": "d11cbd5c",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-01-25T18:38:07.709376Z",
     "start_time": "2025-01-25T18:38:07.678233Z"
    }
   },
   "source": [
    "# generate negative items\n",
    "def generate_negative(data_df,all_items,clicked_item_set,random_seed,neg_item_num=5):\n",
    "    np.random.seed(random_seed)\n",
    "    neg_items = np.random.choice(all_items, (len(data_df),neg_item_num))\n",
    "    for i, uid in tqdm(enumerate(data_df['user_id'].values)):\n",
    "        user_clicked = clicked_item_set[uid]\n",
    "        for j in range(len(neg_items[i])):\n",
    "            while neg_items[i][j] in user_clicked|set(neg_items[i][:j]):\n",
    "                neg_items[i][j] = np.random.choice(all_items, 1)\n",
    "    return neg_items.tolist()\n",
    "\n",
    "clicked_item_set = dict()\n",
    "for user_id, seq_df in all_df.groupby('user_id'):\n",
    "    clicked_item_set[user_id] = set(seq_df['item_id'].values.tolist())\n",
    "all_items = all_df.item_id.unique()\n",
    "val_sel['neg_items'] = generate_negative(val_sel,all_items,clicked_item_set,random_seed=1)\n",
    "test_sel['neg_items'] = generate_negative(test_sel,all_items,clicked_item_set,random_seed=2)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "id": "64009c53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T18:38:07.725214Z",
     "start_time": "2025-01-25T18:38:07.711377Z"
    }
   },
   "source": [
    "select_columns = ['user_id','item_id','time','c_hour_c','c_weekday_c','c_period_c','c_day_f']\n",
    "train[select_columns].to_csv(os.path.join(TOPK_PATH,'train.csv'),sep=\"\\t\",index=False)\n",
    "val_sel[select_columns+['neg_items']].to_csv(os.path.join(TOPK_PATH,'dev.csv'),sep=\"\\t\",index=False)\n",
    "test_sel[select_columns+['neg_items']].to_csv(os.path.join(TOPK_PATH,'test.csv'),sep=\"\\t\",index=False)"
   ],
   "outputs": [],
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "id": "a2e0cafc",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-01-25T18:38:08.798159Z",
     "start_time": "2025-01-25T18:38:07.726214Z"
    }
   },
   "source": [
    "# organize & save item metadata\n",
    "item_meta_train = pd.read_csv(os.path.join(RAW_PATH,'train',\"news.tsv\"),sep=\"\\t\",header=None)\n",
    "item_meta_train.columns = ['news_id','category','subcategory','title','abstract','url','title_entitiy','abstract_entity']\n",
    "item_select = item_meta_train.loc[item_meta_train.news_id.isin(interaction_pos.news_id.unique())].copy()\n",
    "item_select['item_id'] = item_select.news_id.apply(lambda x: item2newid_ctr[x])\n",
    "category2id = dict(zip(sorted(item_select.category.unique()),range(1,item_select.category.nunique()+1)))\n",
    "subcategory2id = dict(zip(sorted(item_select.subcategory.unique()),range(1,item_select.subcategory.nunique()+1)))\n",
    "item_select['i_category_c'] = item_select['category'].apply(lambda x: category2id[x])\n",
    "item_select['i_subcategory_c'] = item_select['subcategory'].apply(lambda x: subcategory2id[x])\n",
    "item_select[['item_id','i_category_c','i_subcategory_c']].to_csv(\n",
    "    os.path.join(TOPK_PATH,'item_meta.csv'),sep=\"\\t\",index=False)"
   ],
   "outputs": [],
   "execution_count": 78
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
