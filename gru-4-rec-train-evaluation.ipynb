{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-01T22:36:25.322840Z",
     "start_time": "2025-02-01T22:32:00.135899Z"
    }
   },
   "source": [
    "from src.experiment import load_mind_data_with_neg\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "#######################################################\n",
    "# 1) Load GRV Data (Item Popularity Over Time)\n",
    "#######################################################\n",
    "\n",
    "def load_cox_data_and_survival(cox_data_csv, cox_survival_csv, itemHourLog_csv):\n",
    "    cox_df = pd.read_csv(cox_data_csv)\n",
    "    if \"T_i0\" in cox_df.columns:\n",
    "        t0_map = dict(zip(cox_df[\"item_id\"], cox_df[\"T_i0\"]))\n",
    "    else:\n",
    "        hour_df = pd.read_csv(itemHourLog_csv)\n",
    "        tmp = hour_df.groupby(\"item_id\")[\"hour_offset\"].min().reset_index()\n",
    "        t0_map = dict(zip(tmp[\"item_id\"], tmp[\"hour_offset\"]))\n",
    "\n",
    "    surv_df = pd.read_csv(cox_survival_csv)\n",
    "    grv_cols = [c for c in surv_df.columns if c.startswith(\"GRV_t\")]\n",
    "    \n",
    "    def parse_off(col):\n",
    "        return int(col.split(\"t\")[-1])\n",
    "\n",
    "    item_grv = {}\n",
    "    for row in surv_df.itertuples(index=False):\n",
    "        it = getattr(row, \"item_id\")\n",
    "        d = {parse_off(c): getattr(row, c) for c in grv_cols}\n",
    "        item_grv[it] = d\n",
    "\n",
    "    cox_map = {it_id: {\"T_i0\": t0_map.get(it_id, 0), \"grv_map\": item_grv[it_id]} for it_id in item_grv}\n",
    "    return cox_map\n",
    "\n",
    "def get_grv(cox_map, item_id, current_hour, default_val=0.0):\n",
    "    if item_id not in cox_map:\n",
    "        return default_val\n",
    "    T0 = cox_map[item_id][\"T_i0\"]\n",
    "    offset = int(current_hour - T0)\n",
    "    if offset <= 0:\n",
    "        return 0.0\n",
    "    grv_map = cox_map[item_id][\"grv_map\"]\n",
    "    offsets = sorted(grv_map.keys())\n",
    "    offset = min(max(offset, offsets[0]), offsets[-1])\n",
    "    return grv_map.get(offset, default_val)\n",
    "\n",
    "#######################################################\n",
    "# 2) GRU4Rec Model (Session-Based Recommendation)\n",
    "#######################################################\n",
    "\n",
    "class GRU4Rec(nn.Module):\n",
    "    def __init__(self, num_items, emb_dim=16, hidden_size=16, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.item_emb = nn.Embedding(num_items, emb_dim)\n",
    "        self.gru = nn.GRU(input_size=emb_dim, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_items)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def forward(self, session_seq):\n",
    "        embedded = self.item_emb(session_seq)\n",
    "        gru_out, _ = self.gru(embedded)\n",
    "        last_out = gru_out[:, -1, :]\n",
    "        last_out = self.dropout(last_out)\n",
    "        return self.fc(last_out)\n",
    "\n",
    "class SessionDataset(Dataset):\n",
    "    def __init__(self, df, item2idx, session_length=5):\n",
    "        self.sessions = []\n",
    "        grouped = df.groupby(\"user_id\")\n",
    "        for _, group in grouped:\n",
    "            group = group.sort_values(\"time\")\n",
    "            items = group[\"item_id\"].values\n",
    "            if len(items) < session_length:\n",
    "                continue\n",
    "            for i in range(len(items) - session_length + 1):\n",
    "                session_seq = items[i: i + session_length]\n",
    "                indices = [item2idx[it] for it in session_seq if it in item2idx]\n",
    "                if len(indices) == session_length:\n",
    "                    self.sessions.append(indices)\n",
    "        self.sessions = np.array(self.sessions)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sessions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        session = self.sessions[idx]\n",
    "        return torch.tensor(session[:-1], dtype=torch.long), torch.tensor(session[-1], dtype=torch.long)\n",
    "\n",
    "#######################################################\n",
    "# 3) Training & Evaluation for GRU4Rec\n",
    "#######################################################\n",
    "\n",
    "def train_one_epoch_gru4rec(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, target in loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)\n",
    "        loss = loss_fn(logits, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def eval_one_epoch_gru4rec(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, target in loader:\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            logits = model(inputs)\n",
    "            loss = loss_fn(logits, target)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == target).sum().item()\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "#######################################################\n",
    "# 4) GRV-Based Ranking for GRU4Rec\n",
    "#######################################################\n",
    "\n",
    "def evaluate_gru4rec_ranking(model, test_df, item2idx, cox_map, gamma=0.3, K=10, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Evaluate GRU4Rec performance using HR@K, NDCG@K, Coverage@K, and New Item Coverage@K.\n",
    "    \"\"\"\n",
    "    test_df = test_df.copy()\n",
    "    test_df[\"time_hr\"] = (test_df[\"time\"] // 3600).astype(int)\n",
    "\n",
    "    grouped = test_df.groupby(\"user_id\")\n",
    "    coverage_items = set()\n",
    "    new_item_hits = 0  # Number of users who received at least one new item\n",
    "\n",
    "    hits_at_k = 0\n",
    "    ndcg_at_k = 0  # ✅ Track cumulative NDCG\n",
    "    total_positives = 0\n",
    "    all_users = list(grouped.groups.keys())\n",
    "    all_item_ids = list(item2idx.keys())\n",
    "\n",
    "    rng = np.random.default_rng(0)\n",
    "\n",
    "    for user_id in tqdm(all_users, desc=\"Evaluating GRU4Rec with GRV\"):\n",
    "        g = grouped.get_group(user_id)\n",
    "        t_hr = g[\"time_hr\"].min()\n",
    "        pos_items = g[g[\"label\"] == 1][\"item_id\"].unique()\n",
    "        candidate_items = np.unique(np.concatenate([pos_items, rng.choice(all_item_ids, size=50, replace=False)]))\n",
    "\n",
    "        valid_candidates = [(it, item2idx[it]) for it in candidate_items if it in item2idx]\n",
    "        if not valid_candidates:\n",
    "            continue\n",
    "\n",
    "        item_indices = [idx for _, idx in valid_candidates]\n",
    "        item_ids = [it for it, _ in valid_candidates]\n",
    "\n",
    "        # Compute model scores\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = torch.tensor(item_indices, dtype=torch.long, device=device).unsqueeze(0)\n",
    "            logits = model(inputs)\n",
    "        base_scores = logits.cpu().numpy().flatten()\n",
    "\n",
    "        # Compute final scores using GRV\n",
    "        final_scores = []\n",
    "        for i, (it, base_score) in enumerate(zip(item_ids, base_scores)):\n",
    "            grv_val = get_grv(cox_map, it, t_hr)\n",
    "            final_score = (1 - gamma) * base_score + gamma * grv_val\n",
    "            final_scores.append(final_score)\n",
    "\n",
    "        # Select top-K items\n",
    "        top_indices = np.argsort(-np.array(final_scores))[:K]\n",
    "        top_items = [item_ids[i] for i in top_indices]\n",
    "\n",
    "        # **Coverage Calculation**\n",
    "        coverage_items.update(top_items)\n",
    "\n",
    "        # **New Item Coverage Calculation**\n",
    "        new_items = set(it for it in top_items if it in cox_map and cox_map[it][\"T_i0\"] >= 100)\n",
    "        if new_items:\n",
    "            new_item_hits += 1  # Count users who received at least one new item\n",
    "\n",
    "        # **HR Calculation**\n",
    "        total_positives += len(pos_items)\n",
    "        hits = sum(1 for pos_it in pos_items if pos_it in top_items)\n",
    "        hits_at_k += hits\n",
    "\n",
    "        # **NDCG Calculation**\n",
    "        dcg = 0.0\n",
    "        for pos_it in pos_items:\n",
    "            if pos_it in top_items:\n",
    "                rank = np.where(np.array(top_items) == pos_it)[0][0] + 1\n",
    "                dcg += 1 / np.log2(rank + 1)  # ✅ Compute DCG\n",
    "\n",
    "        # Ideal DCG (iDCG) - best possible ranking\n",
    "        idcg = sum(1.0 / np.log2(i + 2) for i in range(len(pos_items))) if len(pos_items) > 0 else 0\n",
    "        ndcg_at_k += dcg / idcg if idcg > 0 else 0  # ✅ Normalize DCG\n",
    "\n",
    "    # **Normalize Metrics**\n",
    "    hr = hits_at_k / total_positives if total_positives > 0 else 0\n",
    "    ndcg = ndcg_at_k / len(all_users) if len(all_users) > 0 else 0  # ✅ Normalize by total users\n",
    "    coverage = len(coverage_items) / len(item2idx)  # ✅ Normalize by total items\n",
    "    new_item_coverage = new_item_hits / len(all_users)  # ✅ Normalize by total users\n",
    "\n",
    "    return hr, ndcg, coverage, new_item_coverage\n",
    "\n",
    "#######################################################\n",
    "# 5) Main Experiment for GRU4Rec with GRV\n",
    "#######################################################\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_df, val_df, test_df = load_mind_data_with_neg(\"train.csv\", \"dev.csv\", \"test.csv\")\n",
    "    item2idx = {i: idx for idx, i in enumerate(pd.concat([train_df[\"item_id\"], val_df[\"item_id\"], test_df[\"item_id\"]]).unique())}\n",
    "\n",
    "    train_ds = SessionDataset(train_df, item2idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "\n",
    "    model = GRU4Rec(len(item2idx)).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for ep in range(3):\n",
    "        train_one_epoch_gru4rec(model, train_loader, optimizer, loss_fn, device)\n",
    "\n",
    "    cox_map = load_cox_data_and_survival(\"./cox_output/cox_data.csv\", \"./cox_output/cox_survival.csv\", \"./output/itemHourLog.csv\")\n",
    "\n",
    "    # ✅ FIX: Unpacking the correct number of values\n",
    "    hr, ndcg, cov, newcov = evaluate_gru4rec_ranking(model, test_df, item2idx, cox_map, gamma=0.3, K=10, device=device)\n",
    "\n",
    "    print(f\"[RESULT] HR@10={hr:.4f}, NDCG@10={ndcg:.4f}, coverage@10={cov:.4f}, new_item_coverage@10={newcov:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating GRU4Rec with GRV: 100%|██████████| 57900/57900 [01:17<00:00, 747.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] HR@10=0.2094, NDCG@10=0.1139, coverage@10=0.9992, new_item_coverage@10=0.9834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "806b2582ee585e0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
